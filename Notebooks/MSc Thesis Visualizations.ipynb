{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from io import StringIO # Python3 use: from io import StringIO\n",
    "import seaborn as sns\n",
    "\n",
    "models = tf.keras.models  # like 'from tensorflow.keras import models' (PyCharm import issue workaround)\n",
    "layers = tf.keras.layers  # like 'from tensorflow.keras import layers' (PyCharm import issue workaround)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "DATA = os.path.join(module_path, 'Data', 'Augmented Data')\n",
    "RESULTS = os.path.join(module_path, 'Results', 'Thesis')\n",
    "FIGURES = os.path.join(module_path, 'Figures', 'Thesis')\n",
    "from Scripts import Data_Loader_Functions as DL\n",
    "from Scripts import Model_Architectures as mA\n",
    "from Scripts import Results_Evaluation as rE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_grid(file_paths, y_ticks, metric, top_exp):\n",
    "    colors = ['#CD6155', '#2E86C1']\n",
    "    styles = ['-','--']\n",
    "    \n",
    "    # Create Figure\n",
    "    fig = plt.figure(figsize=(18, 16))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.2)\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        \n",
    "        # Add figure to subplot and read in data frame (session 0 is the 'pre-training session')\n",
    "        \n",
    "        df = pd.read_excel(file_path)\n",
    "        df = df[df['Session'] != 0].reset_index()\n",
    "        \n",
    "        # Prepare dataframe for plotting\n",
    "        metrics = [metric, 'val_' + metric]\n",
    "        df_plot = df[metrics]\n",
    "        df_plot = df_plot.rename(columns={metrics[0]: 'Train ' + metric, metrics[1]: 'Validation ' + metric})\n",
    "        \n",
    "        ax = fig.add_subplot(len(file_paths) // 2 + 1, 2, i + 1)\n",
    "        df_plot.plot(ax=ax, style=styles, color=colors)\n",
    "        \n",
    "        # Add vertical lines indicating sessions\n",
    "        for j, val in enumerate(df[df['Epoch'] == 0].index.values):\n",
    "            ax.axvline(val, c='#C2C5CC', ls=':')\n",
    "            plt.text(val + 1, 0.02, 'S: {}'.format(j+1))\n",
    "            \n",
    "        # Set title and legends\n",
    "        ax.set_title(top_exp[file_path.split('Plotting/')[1]])\n",
    "        ax.set_yticks(y_ticks)\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Centralized Epochs')\n",
    "        else:\n",
    "            ax.set_xlabel('Federated Communication Rounds')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avg_grid(file_paths_mean, file_paths_sd, y_ticks, metric, top_exp):\n",
    "    colors = ['#CD6155', '#2E86C1']\n",
    "    styles = ['-','--']\n",
    "    \n",
    "    # Create Figure\n",
    "    fig = plt.figure(figsize=(18, 16))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.2)\n",
    "    for i, (file_path_1, file_path_2) in enumerate(zip(file_paths_mean, file_paths_sd)):\n",
    "        \n",
    "        # Add figure to subplot and read in data frame (session 0 is the 'pre-training session')\n",
    "        df_mean = pd.read_excel(file_path_1).fillna(0)\n",
    "        df_sd = pd.read_excel(file_path_2).fillna(0)\n",
    "        df_mean = df_mean[df_mean['Session'] != 0].reset_index()\n",
    "        df_sd = df_sd[df_sd['Session'] != 0].reset_index()\n",
    "        \n",
    "        # Prepare dataframe for plotting\n",
    "        metrics = [metric, 'val_' + metric]\n",
    "        df_plot_mean = df_mean[metrics]\n",
    "        df_plot_sd = df_sd[metrics]\n",
    "        df_plot_mean = df_plot_mean.rename(columns={metrics[0]: 'Train ' + metric, metrics[1]: 'Validation ' + metric})\n",
    "        df_plot_sd = df_plot_sd.rename(columns={metrics[0]: 'Train ' + metric, metrics[1]: 'Validation ' + metric})\n",
    "        ax = fig.add_subplot(len(file_paths) // 2 + 1, 2, i + 1)\n",
    "        df_plot_mean.plot(ax=ax, style=styles, color=colors)\n",
    "        plt.fill_between(df_plot_mean.index, (df_plot_mean - df_plot_sd)[df_plot_mean.columns[0]], (df_plot_mean + df_plot_sd)[df_plot_mean.columns[0]], color='#CD6155', alpha=0.2)\n",
    "        plt.fill_between(df_plot_mean.index, (df_plot_mean - df_plot_sd)[df_plot_mean.columns[1]], (df_plot_mean + df_plot_sd)[df_plot_mean.columns[1]], color='#2E86C1', alpha=0.2)\n",
    "        \n",
    "        # Add vertical lines indicating sessions\n",
    "        for j, val in enumerate(df_mean[df_mean['Epoch'] == 0].index.values):\n",
    "            ax.axvline(val, c='#C2C5CC', ls=':')\n",
    "            plt.text(val + 1, 0.02, 'S: {}'.format(j+1))\n",
    "            \n",
    "        # Set title and legends\n",
    "        ax.set_title(top_exp[file_path_1.split('MEAN_')[1]])\n",
    "        ax.set_yticks(y_ticks)\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Centralized Epochs')\n",
    "        else:\n",
    "            ax.set_xlabel('Federated Communication Rounds')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pain_bar_chart(group_by):\n",
    "    # Create Dataframe\n",
    "    df = pd.DataFrame(DL.get_labels(DL.get_image_paths(os.path.join(DATA, 'group_2'))), columns=['Person', 'Session', 'Culture', 'Frame', 'Pain', 'Trans_1', 'Trans2'])\n",
    "    df[['Person', 'Session', 'Culture', 'Frame', 'Pain', ]] = df[['Person', 'Session', 'Culture', 'Frame', 'Pain', ]].astype(int)\n",
    "    \n",
    "    # Get Frequencey values\n",
    "    group = sorted(df[group_by].unique())\n",
    "    share_1 = {}\n",
    "    for elem in group:\n",
    "        key, val = np.unique(df[df[group_by] == elem]['Pain'], return_counts=True)\n",
    "        if 1 in key:\n",
    "            share_1[elem] = val[1] / sum(val[1:])\n",
    "        else:\n",
    "            share_1[elem] = 0\n",
    "    \n",
    "    # Draw figure\n",
    "    plt.figure(figsize=(10,5))\n",
    "    if group_by == 'Session':\n",
    "        bars = plt.bar(share_1.keys(), share_1.values(), color='#2E86C1')\n",
    "        bars[5].set_color('#CD6155')\n",
    "    else:\n",
    "        bars = plt.bar(np.arange(0,len(share_1.keys()),1), share_1.values(), color='#2E86C1')\n",
    "        bars[1].set_color('#CD6155')\n",
    "    autolabel(bars)\n",
    "    \n",
    "    \n",
    "    plt.xticks(np.arange(0,len(share_1.keys())+1,1), group)\n",
    "    plt.yticks(np.arange(0,1.1,0.2), ['{}%'.format(int(num * 100)) for num in np.arange(0,1.1,0.2)])\n",
    "    plt.xlabel(group_by) if group_by == 'Session' else plt.xlabel(\"Test Subjects\")\n",
    "    plt.ylabel(\"Share of Pain Level '1'\")\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.grid(True, linestyle='--')\n",
    "    fig = plt.gcf()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avg_conf_matrix(model_type, session, subject):\n",
    "    df_concat = pd.DataFrame()\n",
    "    for folder in folders:\n",
    "        f_path = os.path.join(RESULTS, folder)\n",
    "        files = [os.path.join(f_path, file) for file in os.listdir(f_path) if '.csv' in file and model_type in file]\n",
    "        df_concat = df_concat.add(pd.read_csv(files[0]), fill_value=0)\n",
    "    df_concat = df_concat / len(folders)\n",
    "\n",
    "    cols = [col for col in df_concat.columns if str(subject) in col]\n",
    "    df_conf_matrix = pd.DataFrame()\n",
    "    df_conf_matrix['0'] = [df_concat[cols].iloc[session - 1].loc['subject_{}_true_negatives'.format(subject)],\n",
    "                           df_concat[cols].iloc[session - 1].loc['subject_{}_false_positives'.format(subject)]]\n",
    "    df_conf_matrix['1'] = [df_concat[cols].iloc[session - 1].loc['subject_{}_false_negatives'.format(subject)],\n",
    "                           df_concat[cols].iloc[session - 1].loc['subject_{}_true_positives'.format(subject)]]\n",
    "    return df_conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolabel(rects, xpos='center'):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar in *rects*, displaying its height.\n",
    "\n",
    "    *xpos* indicates which side to place the text w.r.t. the center of\n",
    "    the bar. It can be one of the following {'center', 'right', 'left'}.\n",
    "    \"\"\"\n",
    "\n",
    "    ha = {'center': 'center', 'right': 'left', 'left': 'right'}\n",
    "    offset = {'center': 0, 'right': 1, 'left': -1}\n",
    "\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        plt.annotate('{:.0%}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(offset[xpos]*3, 3),  # use 3 points offset\n",
    "                    textcoords=\"offset points\",  # in both directions\n",
    "                    ha=ha[xpos], va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pain_df(path, pain_gap=()):\n",
    "    img_paths = np.array(DL.get_image_paths(path))\n",
    "    labels = np.array(DL.get_labels(img_paths))\n",
    "    df = pd.DataFrame(labels, columns=['Person', 'Session', 'Culture', 'Frame', 'Pain', 'Trans_1', 'Trans_2'])\n",
    "    df[['Person', 'Session', 'Culture', 'Frame', 'Pain']] = df[\n",
    "        ['Person', 'Session', 'Culture', 'Frame', 'Pain']].astype(int)\n",
    "    df['img_path'] = img_paths\n",
    "    df[['Trans_1', 'Trans_2', 'img_path']] = df[['Trans_1', 'Trans_2', 'img_path']].astype(str)\n",
    "    df = df.sort_values(['Person', 'Session', 'Frame', 'Trans_1', 'Trans_2'],\n",
    "                        ascending=[True, True, True, False, False]).reset_index(drop=True)\n",
    "    df['temp_id'] = df['Person'].astype(str) + df['Session'].astype(str) + df['Frame'].astype(str)\n",
    "    df = df[~df['Pain'].isin(pain_gap)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary_to_df(model, mystdout):\n",
    "    model.summary(line_length=100)\n",
    "    model_summary = mystdout.getvalue()\n",
    "    a = model_summary.split('\\n')\n",
    "    a = a[4:-5]\n",
    "    a = a[::2]\n",
    "    for idx, elem in enumerate(a):\n",
    "        a[idx] = [elem[:45], elem[45:85], elem[85:]]\n",
    "    return pd.DataFrame(a, columns=['Layer Type', 'Output Shape', 'Param #'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df):\n",
    "    df['Layer Type'] = df['Layer Type'].map(lambda x: x.split('(')[1].split(')')[0])\n",
    "    df['Param #'] = df['Param #'].astype(float)\n",
    "    df = df.append(df.sum(numeric_only=True), ignore_index=True)\n",
    "    df.loc[len(df)-1, 'Layer Type'] = 'Total'\n",
    "    df['Param #'] = df['Param #'].apply(\"{0:,.0f}\".format)\n",
    "    return df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary():\n",
    "    model_1 = build_CNN((215, 215, 1))\n",
    "    old_stdout = copy.copy(sys.stdout)\n",
    "    sys.stdout = mystdout = StringIO()\n",
    "    df = model_summary_to_df(model_1, mystdout)\n",
    "    df = format_df(df)\n",
    "    df = pd.concat((pd.DataFrame([{'Layer Type': 'Input', 'Output Shape': '(None, 215, 215, 1)',  'Param #': 0}]), df), ignore_index=True)\n",
    "    return df, old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN(input_shape):\n",
    "    \"\"\"\n",
    "    Return a simple CNN model for image classification.\n",
    "\n",
    "    :param input_shape:     image input shape (tuple), e.g. (28, 28, 1)\n",
    "\n",
    "    :return:\n",
    "        model               compiled tensorflow model\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Setting up CNN\")\n",
    "    # Set up model type\n",
    "    model = models.Sequential(name='CNN')\n",
    "\n",
    "    # Add layers\n",
    "    model.add(layers.Conv2D(filters=32, kernel_size=(5, 5), input_shape=input_shape, padding='same', strides=(2, 2),\n",
    "                            name='conv2d_0_global'))\n",
    "    model.add(layers.BatchNormalization(name='batch_norm_0_global'))\n",
    "    model.add(layers.ReLU(name='relu_0_global'))\n",
    "    # model.add(layers.MaxPooling2D(name='max_pool_0_global'))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size=(5, 5), padding='same', strides=(2, 2), name='conv2d_1_global'))\n",
    "    model.add(layers.BatchNormalization(name='batch_norm_1_global'))\n",
    "    model.add(layers.ReLU(name='relu_1_global'))\n",
    "    # model.add(layers.MaxPooling2D(name='max_pool_1_global'))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=128, kernel_size=(5, 5), padding='same', strides=(2, 2), name='conv2d_2_global'))\n",
    "    model.add(layers.BatchNormalization(name='batch_norm_2_global'))\n",
    "    model.add(layers.ReLU(name='relu_2_global'))\n",
    "    model.add(layers.MaxPooling2D(name='max_pool_2_global'))\n",
    "\n",
    "    model.add(layers.Flatten(name='flatten_0_local'))\n",
    "    model.add(layers.Dense(units=128, name='dense_0_local'))\n",
    "    model.add(layers.BatchNormalization(name='batch_norm_3_local'))\n",
    "    model.add(layers.ReLU(name='relu_3_local'))\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid', name='dense_1_local'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_pain_df(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df[(df['Trans_1'] == 'original') & (df['Trans_2'] == 'straight')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48106"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pain Distribution - Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rlr}\n",
      "\\toprule\n",
      " Pain Level &   Count &     \\% \\\\\n",
      "\\midrule\n",
      "          0 &  39,846 &  82.8 \\\\\n",
      "          1 &   3,100 &   6.4 \\\\\n",
      "          2 &   2,281 &   4.7 \\\\\n",
      "          3 &   1,408 &   2.9 \\\\\n",
      "          4 &     801 &   1.7 \\\\\n",
      "          5 &     241 &   0.5 \\\\\n",
      "          6 &     265 &   0.6 \\\\\n",
      "          7 &      53 &   0.1 \\\\\n",
      "          8 &      79 &   0.2 \\\\\n",
      "          9 &      32 &   0.1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pain_levels = pd.DataFrame(np.unique(df_original['Pain'], return_counts=True)).T.rename(columns={0: 'Pain Level', 1: \"Count\"})\n",
    "df_pain_levels['%'] = round(df_pain_levels['Count'] / df_pain_levels['Count'].sum(), 3) * 100\n",
    "df_pain_levels['Count'] = df_pain_levels['Count'].apply(\"{0:,.0f}\".format)\n",
    "print(df_pain_levels.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram Equilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PLT Parameters\n",
    "plt.rcParams.update({'font.size': 14, \n",
    "                     'font.family' : 'cmr10', \n",
    "                     'font.weight' : 'normal',\n",
    "                     'axes.titlesize' : 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2c66b1b41711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dummy Data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'47_0_0_0_0_grey.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhist_eq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dummy Data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'47_0_0_0_0_hist.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "grey = cv2.imread(os.path.join('Dummy Data', '47_0_0_0_0_grey.png'), 0).flatten()\n",
    "hist_eq = cv2.imread(os.path.join('Dummy Data', '47_0_0_0_0_hist.jpg'), 0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grey' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d50776772721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Frequency Count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pixel Value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFIGURES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'histogram.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grey' is not defined"
     ]
    }
   ],
   "source": [
    "ax1 = plt.hist(grey, 256)\n",
    "plt.ylabel('Frequency Count')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.savefig(os.path.join(FIGURES, 'histogram.pdf'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_eq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-12fde2f89ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_eq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Frequency Count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pixel Value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFIGURES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'histogram_eq.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist_eq' is not defined"
     ]
    }
   ],
   "source": [
    "ax1 = plt.hist(hist_eq, 256)\n",
    "plt.ylabel('Frequency Count')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.savefig(os.path.join(FIGURES, 'histogram_eq.pdf'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pain vs. No Pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39846 | 8260\n"
     ]
    }
   ],
   "source": [
    "print(sum(df_original['Pain'] == 0),\"|\", sum(df_original['Pain'] > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 1 vs. Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102976 | 78180\n"
     ]
    }
   ],
   "source": [
    "df_1 = create_pain_df(os.path.join(DATA, \"group_1\"))\n",
    "df_2 = create_pain_df(os.path.join(DATA, \"group_2\"))\n",
    "print(len(df_1), \"|\", len(df_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "Group 1 &  42 &  47 &  49 &  66 &  95 &  97 &  103 &  106 &  108 &  121 &  123 &  124 \\\\\n",
      "Group 2 &  43 &  48 &  52 &  59 &  64 &  80 &   92 &   96 &  107 &  109 &  115 &  120 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p_1 = pd.DataFrame(df_1['Person'].unique(), columns=['Group 1'])\n",
    "df_p_2 = pd.DataFrame(df_2['Person'].unique(), columns=['Group 2'])\n",
    "df_p = pd.concat((df_p_1, df_p_2), sort=False, axis=1).T\n",
    "print(df_p.to_latex(header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 42,  43,  47,  48,  49,  52,  59,  64,  66,  80,  92,  95,  96,\n",
       "        97, 101, 103, 106, 107, 108, 109, 115, 120, 121, 123, 124])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Person'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rlll}\n",
      "\\toprule\n",
      "Group 1 \\\\\n",
      " Person & No Pain & Pain & \\% Pain \\\\\n",
      "\\midrule\n",
      "     42 &   1,895 &  239 &    11\\% \\\\\n",
      "     47 &   1,544 &   64 &     3\\% \\\\\n",
      "     49 &   2,194 &  524 &    19\\% \\\\\n",
      "     66 &   1,947 &  512 &    20\\% \\\\\n",
      "     95 &     304 &  498 &    62\\% \\\\\n",
      "     97 &   3,212 &  147 &     4\\% \\\\\n",
      "    103 &   2,738 &  824 &    23\\% \\\\\n",
      "    106 &   2,281 &  517 &    18\\% \\\\\n",
      "    108 &   2,453 &  455 &    15\\% \\\\\n",
      "    121 &     478 &   40 &     7\\% \\\\\n",
      "    123 &     822 &  361 &    30\\% \\\\\n",
      "    124 &     699 &  996 &    58\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{rlll}\n",
      "\\toprule\n",
      "Group 2 \\\\\n",
      " Person & No Pain &   Pain & \\% Pain \\\\\n",
      "\\midrule\n",
      "     43 &   1,028 &     92 &     8\\% \\\\\n",
      "     48 &     798 &     84 &     9\\% \\\\\n",
      "     52 &   2,503 &    106 &     4\\% \\\\\n",
      "     59 &     640 &    133 &    17\\% \\\\\n",
      "     64 &   1,394 &    155 &    10\\% \\\\\n",
      "     80 &     896 &  1,068 &    54\\% \\\\\n",
      "     92 &   1,031 &    471 &    31\\% \\\\\n",
      "     96 &   2,175 &    178 &     7\\% \\\\\n",
      "    107 &   1,599 &    442 &    21\\% \\\\\n",
      "    109 &   1,724 &    179 &     9\\% \\\\\n",
      "    115 &   1,184 &     99 &     7\\% \\\\\n",
      "    120 &   1,490 &     76 &     4\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_original = df_original.copy()\n",
    "df_original['Pain'] = np.minimum(df_original['Pain'], 1)\n",
    "no_pain = df_original.groupby('Person')['Pain'].count() - df_original.groupby('Person')['Pain'].sum()\n",
    "pain = df_original.groupby('Person')['Pain'].sum()\n",
    "no_pain.name = 'No Pain'\n",
    "pain_df = pd.concat((pd.DataFrame(no_pain), pd.DataFrame(pain)), axis=1)\n",
    "pain_df['% Pain'] = (pain_df['Pain'] / pain_df.sum(axis=1) * 100).astype(int).astype(str) + '%'\n",
    "pain_df = pain_df.reset_index()\n",
    "pain_df['Pain'] = pain_df['Pain'].apply(\"{0:,.0f}\".format)\n",
    "pain_df['No Pain'] = pain_df['No Pain'].apply(\"{0:,.0f}\".format)\n",
    "pain_1 = pain_df[pain_df['Person'].isin(df_p_1['Group 1'])]\n",
    "pain_2 = pain_df[pain_df['Person'].isin(df_p_2['Group 2'])]\n",
    "array = [['Group 1', 'Group 1', 'Group 1', 'Group 1'], list(pain_1.columns.values)]\n",
    "tuples = list(zip(*array))\n",
    "idx1 = pd.MultiIndex.from_tuples(tuples)\n",
    "array = [['Group 2', 'Group 2', 'Group 2', 'Group 2'], list(pain_2.columns.values)]\n",
    "tuples = list(zip(*array))\n",
    "idx2 = pd.MultiIndex.from_tuples(tuples)\n",
    "pain_1.columns = idx1\n",
    "pain_2.columns = idx2\n",
    "print(pain_1.to_latex(index=False))\n",
    "print(pain_2.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pain Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rllllllllllrlll}\n",
      "\\toprule\n",
      "       & \\multicolumn{10}{l}{Session} & \\multicolumn{4}{l}{Total} \\\\\n",
      "Person &       0 &    1 &    2 &    3 &    4 &    5 &    6 & 7 &    8 &    9 & \\# of Sessions &   Pain & No Pain & Pain \\% \\\\\n",
      "\\midrule\n",
      "    43 &     140 &      &      &      &  228 &      &      &   &      &      &             9 &    368 &   4,112 &     8\\% \\\\\n",
      "    48 &         &  148 &      &      &      &  188 &      &   &      &      &             7 &    336 &   3,192 &    10\\% \\\\\n",
      "    52 &      72 &      &      &      &      &      &   44 &   &  120 &  188 &            10 &    424 &  10,012 &     4\\% \\\\\n",
      "    59 &         &  532 &      &      &      &      &      &   &      &      &             2 &    532 &   2,560 &    17\\% \\\\\n",
      "    64 &     244 &   64 &   64 &      &  248 &      &      &   &      &      &             6 &    620 &   5,576 &    10\\% \\\\\n",
      "    80 &   1,052 &  536 &  484 &  484 &  660 &  792 &  264 &   &      &      &             7 &  4,272 &   3,584 &    54\\% \\\\\n",
      "    92 &     464 &  696 &      &      &  724 &      &      &   &      &      &             5 &  1,884 &   4,124 &    31\\% \\\\\n",
      "    96 &         &      &      &      &  112 &      &  512 &   &   88 &      &             9 &    712 &   8,700 &     8\\% \\\\\n",
      "   107 &      32 &      &  848 &   60 &  828 &      &      &   &      &      &             8 &  1,768 &   6,396 &    22\\% \\\\\n",
      "   109 &         &  600 &      &      &      &  116 &      &   &      &      &             8 &    716 &   6,896 &     9\\% \\\\\n",
      "   115 &      60 &      &  220 &   56 &   60 &      &      &   &      &      &             5 &    396 &   4,736 &     8\\% \\\\\n",
      "   120 &     116 &      &      &  188 &      &      &      &   &      &      &             8 &    304 &   5,960 &     5\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 'Person'\n",
    "columns = 'Session'\n",
    "values = index\n",
    "pivot = DL.create_pivot(os.path.join(DATA, \"group_2\"), index, columns, values)\n",
    "pivot['Pain %'] = (pivot['Pain %'] * 100).astype(int).astype(str) + '%'\n",
    "pivot = pivot.drop('Total')\n",
    "pivot = pivot.reset_index()\n",
    "array = [[' ', 'Session', 'Session', 'Session', 'Session', 'Session', 'Session', 'Session', 'Session', 'Session', 'Session', 'Total', 'Total', 'Total', 'Total'], list(pivot.columns.values)]\n",
    "tuples = list(zip(*array))\n",
    "index = pd.MultiIndex.from_tuples(tuples)\n",
    "pivot.columns = index\n",
    "pivot = pivot.sort_values([(' ', 'Person')], ascending=True)\n",
    "pivot['Session'] = pivot['Session'][pivot['Session'] != ''].astype(float).applymap(\"{0:,.0f}\".format).replace('nan', '')\n",
    "pivot[('Total', 'Pain')] = pivot[('Total', 'Pain')].astype(float).apply(\"{0:,.0f}\".format)\n",
    "pivot[('Total', 'No Pain')] = pivot[('Total', 'No Pain')].astype(float).apply(\"{0:,.0f}\".format)\n",
    "print(pivot.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up CNN\n",
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "         Layer Type &                              Output Shape &    Param \\# \\\\\n",
      "\\midrule\n",
      "              Input &                       (None, 215, 215, 1) &          0 \\\\\n",
      "             Conv2D &  (None, 108, 108, 32)                     &        832 \\\\\n",
      " BatchNormalization &  (None, 108, 108, 32)                     &        128 \\\\\n",
      "               ReLU &  (None, 108, 108, 32)                     &          0 \\\\\n",
      "             Conv2D &  (None, 54, 54, 64)                       &     51,264 \\\\\n",
      " BatchNormalization &  (None, 54, 54, 64)                       &        256 \\\\\n",
      "               ReLU &  (None, 54, 54, 64)                       &          0 \\\\\n",
      "             Conv2D &  (None, 27, 27, 128)                      &    204,928 \\\\\n",
      " BatchNormalization &  (None, 27, 27, 128)                      &        512 \\\\\n",
      "               ReLU &  (None, 27, 27, 128)                      &          0 \\\\\n",
      "       MaxPooling2D &  (None, 13, 13, 128)                      &          0 \\\\\n",
      "            Flatten &  (None, 21632)                            &          0 \\\\\n",
      "              Dense &  (None, 128)                              &  2,769,024 \\\\\n",
      " BatchNormalization &  (None, 128)                              &        512 \\\\\n",
      "               ReLU &  (None, 128)                              &          0 \\\\\n",
      "              Dense &  (None, 1)                                &        129 \\\\\n",
      "              Total &                                           &  3,027,585 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PADDING = 'same'\n",
    "BATCH_NORM = True\n",
    "RELU = True\n",
    "MAX_POOL = True\n",
    "GLOB_MAX_POOL = True\n",
    "STRIDE = (1, 1)\n",
    "\n",
    "df, stdout = model_summary()\n",
    "sys.stdout = stdout\n",
    "print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Variables\n",
    "pivot = DL.create_pivot(os.path.join(DATA, 'group_2'), 'Session', 'Person', 'Session')\n",
    "subjects = DL.create_pain_df(os.path.join(DATA, 'group_2'))['Person'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment names\n",
    "exp_names = {\n",
    "    '0-sessions-Baseline-random': 'RANDOM',\n",
    "    '0-sessions-Baseline-central-pre-training': 'BC-CNN',\n",
    "    '0-sessions-Baseline-federated-pre-training': 'BF-CNN',\n",
    "    '1-sessions-Centralized-no-pre-training': 'C-CNN (N)',\n",
    "    '10-sessions-Federated-central-pre-training-local-models': 'FL-CNN (C)',\n",
    "    '11-sessions-Federated-federated-pre-training-local-models': 'FL-CNN (F)',\n",
    "    '2-sessions-Centralized-pre-training': 'C-CNN (C)',\n",
    "    '3-sessions-Federated-no-pre-training': 'F-CNN (N)',\n",
    "    '4-sessions-Federated-central-pre-training': 'F-CNN (C)',\n",
    "    '5-sessions-Federated-federated-pre-training': 'F-CNN (F)',\n",
    "    '6-sessions-Federated-no-pre-training-personalization': 'FP-CNN (N)',\n",
    "    '7-sessions-Federated-central-pre-training-personalization': 'FP-CNN (C)',\n",
    "    '8-sessions-Federated-federated-pre-training-personalization': 'FP-CNN (F)',\n",
    "    '9-sessions-Federated-no-pre-training-local-models': 'FL-CNN (N)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top experiments\n",
    "top_exp = ['RANDOM', 'BC-CNN', 'C-CNN (C)', 'F-CNN (C)', 'FP-CNN (C)', 'FL-CNN (C)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rE.compute_average_metrics('person', subjects, pivot, RESULTS)\n",
    "overview_table = rE.generate_overview_table(results, exp_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "            & \\multicolumn{3}{l}{Weighted AVG + STD} \\\\\n",
      " Experiment &                ACC &   PR-AUC &       F1 \\\\\n",
      "\\midrule\n",
      "     BC-CNN &            73 ± 12 &  54 ± 23 &  47 ± 24 \\\\\n",
      "     BF-CNN &            74 ± 12 &  53 ± 23 &  43 ± 21 \\\\\n",
      "     RANDOM &            44 ± 15 &  31 ± 16 &   32 ± 2 \\\\\n",
      "  C-CNN (N) &            69 ± 17 &  49 ± 23 &  39 ± 25 \\\\\n",
      "  C-CNN (C) &            75 ± 13 &  58 ± 21 &  50 ± 22 \\\\\n",
      "  F-CNN (N) &            66 ± 16 &  49 ± 23 &  43 ± 27 \\\\\n",
      "  F-CNN (C) &            75 ± 11 &  59 ± 23 &  52 ± 25 \\\\\n",
      "  F-CNN (F) &            76 ± 12 &  59 ± 23 &  49 ± 25 \\\\\n",
      " FP-CNN (N) &            69 ± 18 &  43 ± 19 &  34 ± 25 \\\\\n",
      " FP-CNN (C) &            76 ± 12 &  56 ± 21 &  50 ± 24 \\\\\n",
      " FP-CNN (F) &            76 ± 13 &  55 ± 22 &  44 ± 24 \\\\\n",
      " FL-CNN (N) &            69 ± 18 &  43 ± 18 &  34 ± 26 \\\\\n",
      " FL-CNN (C) &            75 ± 13 &  55 ± 21 &  47 ± 23 \\\\\n",
      " FL-CNN (F) &            75 ± 14 &  54 ± 21 &  42 ± 23 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(overview_table.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "            & \\multicolumn{3}{l}{Weighted AVG + STD} \\\\\n",
      " Experiment &                ACC &   PR-AUC &       F1 \\\\\n",
      "\\midrule\n",
      "     BC-CNN &            73 ± 12 &  54 ± 23 &  47 ± 24 \\\\\n",
      "     RANDOM &            44 ± 15 &  31 ± 16 &   32 ± 2 \\\\\n",
      "  C-CNN (C) &            75 ± 13 &  58 ± 21 &  50 ± 22 \\\\\n",
      "  F-CNN (C) &            75 ± 11 &  59 ± 23 &  52 ± 25 \\\\\n",
      " FP-CNN (C) &            76 ± 12 &  56 ± 21 &  50 ± 24 \\\\\n",
      " FL-CNN (C) &            75 ± 13 &  55 ± 21 &  47 ± 23 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top experiments\n",
    "print(overview_table[overview_table[('', 'Experiment')].isin(top_exp)].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textbf{ACC} &  \\textbf{1} &  \\textbf{2} &  \\textbf{3} &  \\textbf{4} &  \\textbf{5} &  \\textbf{6} &  \\textbf{7} &  \\textbf{8} &  \\textbf{9} &  \\textbf{wt. Mean ± SD} &  \n"
     ]
    }
   ],
   "source": [
    "# Generate Header for LaTex table\n",
    "# header = ['ACC',  43,  48,  52,  59,  64,  80,  92,  96,  107,  109,  115,  120, 'wt. Mean ± SD']\n",
    "header = ['ACC',  1, 2, 3, 4, 5, 6, 7, 8, 9, 'wt. Mean ± SD']\n",
    "formatter = ['\\\\textbf{' + '{}'.format(elem) + '}' for elem in header]\n",
    "formatted_header = ''\n",
    "for elem in formatter:\n",
    "    formatted_header = formatted_header + elem + ' &  '\n",
    "print(formatted_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p = rE.compute_average_metrics('person', subjects, pivot, RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrrl}\n",
      "\\toprule\n",
      " Experiment &  43 &  48 &  52 &  59 &  64 &  80 &  92 &  96 &  107 &  109 &  115 &  120 & Mean ± SD \\\\\n",
      "\\midrule\n",
      "     BC-CNN &  70 &  78 &  92 &  48 &  90 &  57 &  68 &  79 &   64 &   76 &   70 &   72 &   72 ± 13 \\\\\n",
      "     RANDOM &  44 &  44 &  38 &  46 &  37 &  49 &  53 &  41 &   49 &   41 &   38 &   45 &   44 ± 16 \\\\\n",
      "  C-CNN (C) &  79 &  78 &  81 &  48 &  91 &  62 &  72 &  84 &   68 &   77 &   88 &   67 &   75 ± 12 \\\\\n",
      "  F-CNN (C) &  71 &  78 &  91 &  48 &  92 &  61 &  72 &  84 &   69 &   71 &   74 &   66 &   73 ± 13 \\\\\n",
      " FP-CNN (C) &  82 &  78 &  87 &  48 &  91 &  62 &  78 &  84 &   67 &   77 &   90 &   73 &   76 ± 13 \\\\\n",
      " FL-CNN (C) &  83 &  78 &  86 &  48 &  92 &  61 &  66 &  84 &   68 &   77 &   90 &   71 &   75 ± 13 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrrrrrrrl}\n",
      "\\toprule\n",
      " Experiment &  43 &  48 &  52 &  59 &  64 &  80 &  92 &  96 &  107 &  109 &  115 &  120 & Mean ± SD \\\\\n",
      "\\midrule\n",
      "     BC-CNN &  70 &  27 &  39 &  62 &  39 &  65 &  80 &  47 &   67 &   42 &   55 &   70 &   55 ± 17 \\\\\n",
      "     RANDOM &  36 &  21 &   8 &  36 &  12 &  49 &  64 &  24 &   46 &   29 &   12 &   36 &   31 ± 17 \\\\\n",
      "  C-CNN (C) &  74 &  26 &  39 &  62 &  46 &  70 &  85 &  52 &   74 &   48 &   55 &   65 &   58 ± 17 \\\\\n",
      "  F-CNN (C) &  75 &  28 &  46 &  62 &  46 &  70 &  85 &  54 &   79 &   44 &   52 &   65 &   59 ± 17 \\\\\n",
      " FP-CNN (C) &  79 &  27 &  40 &  62 &  42 &  64 &  87 &  49 &   78 &   45 &   53 &   65 &   58 ± 18 \\\\\n",
      " FL-CNN (C) &  78 &  26 &  39 &  62 &  41 &  63 &  83 &  48 &   77 &   46 &   49 &   62 &   56 ± 18 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrrrrrrrl}\n",
      "\\toprule\n",
      " Experiment &  43 &  48 &  52 &  59 &  64 &  80 &  92 &  96 &  107 &  109 &  115 &  120 & Mean ± SD \\\\\n",
      "\\midrule\n",
      "     BC-CNN &  24 &   5 &  32 &  56 &  25 &  60 &  79 &   8 &   59 &   30 &   35 &   65 &   40 ± 24 \\\\\n",
      "     RANDOM &  35 &  24 &  10 &  37 &  14 &  44 &  51 &  25 &   42 &   31 &   15 &   36 &   30 ± 13 \\\\\n",
      "  C-CNN (C) &  64 &   7 &  30 &  56 &  39 &  53 &  67 &  45 &   62 &   32 &   46 &   61 &   47 ± 18 \\\\\n",
      "  F-CNN (C) &  27 &   7 &  44 &  56 &  25 &  58 &  81 &  46 &   63 &   42 &   35 &   62 &   46 ± 20 \\\\\n",
      " FP-CNN (C) &  72 &   7 &  36 &  56 &  37 &  52 &  76 &  35 &   68 &   29 &   47 &   48 &   47 ± 20 \\\\\n",
      " FL-CNN (C) &  75 &   7 &  35 &  56 &  34 &  49 &  53 &  37 &   68 &   33 &   42 &   46 &   45 ± 18 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rE.prepare_top_experiments(results_p['accuracy'], exp_names, top_exp).to_latex(index=False))\n",
    "print(rE.prepare_top_experiments(results_p['pr'], exp_names, top_exp).to_latex(index=False))\n",
    "tab = rE.prepare_top_experiments(results_p['f1_score'], exp_names, top_exp)\n",
    "cols = [col for col in results_p['f1_score'].columns.values if type(col) is int]\n",
    "tab['Mean ± SD'] = tab[cols].mean(axis=1).round(0).astype(int).astype(str) + ' ± ' + tab[cols].std(axis=1).round(0).astype(int).astype(str)\n",
    "print(tab.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['wt. Mean ± SD'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7c309d5fc6be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_top_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Experiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wt. Mean ± SD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dense'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_pr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PR-AUC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_top_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Experiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wt. Mean ± SD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dense'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'F1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_top_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f1_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Experiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wt. Mean ± SD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dense'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_pr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_latex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/FederatedLearning/venv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/PycharmProjects/FederatedLearning/venv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/FederatedLearning/venv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/FederatedLearning/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['wt. Mean ± SD'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df_acc = pd.DataFrame(columns=['Accuracy'], data=(rE.prepare_top_experiments(results_p['accuracy'], exp_names, top_exp).set_index('Experiment').drop('wt. Mean ± SD', axis=1).rank(method='dense') - 1).sum(axis=1)) / 12\n",
    "df_pr = pd.DataFrame(columns=['PR-AUC'], data=(rE.prepare_top_experiments(results_p['pr'], exp_names, top_exp).set_index('Experiment').drop('wt. Mean ± SD', axis=1).rank(method='dense') - 1).sum(axis=1)) / 12\n",
    "df_f1 = pd.DataFrame(columns=['F1'], data=(rE.prepare_top_experiments(results_p['f1_score'], exp_names, top_exp).set_index('Experiment').drop('wt. Mean ± SD', axis=1).rank(method='dense') - 1).sum(axis=1)) / 12\n",
    "print(pd.concat((df_acc, df_pr, df_f1), axis=1).reset_index().to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_s = rE.compute_average_metrics('session', subjects, pivot, RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrlrrl}\n",
      "\\toprule\n",
      " Experiment &   1 &   2 &   3 &   4 &   5 &   6 &   7 &   8 &   9 & Mean ± SD \\\\\n",
      "\\midrule\n",
      "     BC-CNN &  68 &  63 &  77 &  72 &  68 &  69 &  NA &  89 &  96 &   75 ± 12 \\\\\n",
      "     RANDOM &  47 &  43 &  44 &  45 &  44 &  44 &  NA &  37 &  39 &   43 ± 12 \\\\\n",
      "  C-CNN (C) &  68 &  74 &  82 &  78 &  64 &  79 &  NA &  81 &  81 &    76 ± 7 \\\\\n",
      "  F-CNN (C) &  68 &  73 &  78 &  74 &  61 &  80 &  NA &  88 &  96 &   77 ± 11 \\\\\n",
      " FP-CNN (C) &  68 &  76 &  83 &  78 &  65 &  80 &  NA &  85 &  89 &    78 ± 8 \\\\\n",
      " FL-CNN (C) &  68 &  74 &  84 &  75 &  64 &  79 &  NA &  84 &  88 &    77 ± 8 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrlrrl}\n",
      "\\toprule\n",
      " Experiment &   1 &   2 &   3 &   4 &   5 &   6 &   7 &   8 &   9 & Mean ± SD \\\\\n",
      "\\midrule\n",
      "     BC-CNN &  53 &  56 &  51 &  73 &  39 &  48 &  NA &  10 &  80 &   51 ± 22 \\\\\n",
      "     RANDOM &  41 &  34 &  26 &  36 &  35 &  28 &  NA &   7 &  10 &   27 ± 13 \\\\\n",
      "  C-CNN (C) &  53 &  68 &  56 &  78 &  38 &  48 &  NA &  14 &  79 &   54 ± 22 \\\\\n",
      "  F-CNN (C) &  53 &  65 &  60 &  79 &  39 &  56 &  NA &  12 &  91 &   57 ± 24 \\\\\n",
      " FP-CNN (C) &  53 &  62 &  63 &  76 &  37 &  42 &  NA &  13 &  78 &   53 ± 22 \\\\\n",
      " FL-CNN (C) &  53 &  60 &  61 &  74 &  37 &  40 &  NA &  13 &  77 &   52 ± 21 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrrlrrl}\n",
      "\\toprule\n",
      " Experiment &   1 &   2 &   3 &   4 &   5 &   6 &   7 &   8 &   9 & Mean ± SD \\\\\n",
      "\\midrule\n",
      "     BC-CNN &  49 &  50 &  49 &  66 &  12 &  36 &  NA &   3 &  75 &   42 ± 25 \\\\\n",
      "     RANDOM &  40 &  34 &  28 &  36 &  35 &  29 &  NA &   9 &  13 &   28 ± 11 \\\\\n",
      "  C-CNN (C) &  49 &  56 &  50 &  66 &   5 &  29 &  NA &  28 &  50 &   42 ± 20 \\\\\n",
      "  F-CNN (C) &  49 &  54 &  51 &  68 &  24 &  41 &  NA &  15 &  80 &   48 ± 21 \\\\\n",
      " FP-CNN (C) &  49 &  55 &  56 &  69 &   5 &  25 &  NA &  26 &  60 &   43 ± 22 \\\\\n",
      " FL-CNN (C) &  49 &  48 &  54 &  62 &   8 &  26 &  NA &  26 &  58 &   41 ± 19 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rE.prepare_top_experiments(results_s['accuracy'], exp_names, top_exp).to_latex(index=False))\n",
    "print(rE.prepare_top_experiments(results_s['pr'], exp_names, top_exp).to_latex(index=False))\n",
    "tab = rE.prepare_top_experiments(results_s['f1_score'], exp_names, top_exp)\n",
    "cols = [col for col in results_s['f1_score'].columns.values if type(col) is int]\n",
    "tab['Mean ± SD'] = tab[cols].mean(axis=1).round(0).astype(int).astype(str) + ' ± ' + tab[cols].std(axis=1).round(0).astype(int).astype(str)\n",
    "print(tab.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BC-CNN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RANDOM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C-CNN (C)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F-CNN (C)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP-CNN (C)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL-CNN (C)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             1   2   3   4   5   6   7   8   9\n",
       "Experiment                                    \n",
       "BC-CNN     1.0 1.0 1.0 1.0 4.0 1.0 0.0 5.0 4.0\n",
       "RANDOM     0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
       "C-CNN (C)  1.0 3.0 3.0 4.0 2.0 2.0 0.0 1.0 1.0\n",
       "F-CNN (C)  1.0 2.0 2.0 2.0 1.0 3.0 0.0 4.0 4.0\n",
       "FP-CNN (C) 1.0 4.0 4.0 4.0 3.0 3.0 0.0 3.0 3.0\n",
       "FL-CNN (C) 1.0 3.0 5.0 3.0 2.0 2.0 0.0 2.0 2.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rE.prepare_top_experiments(results_s['accuracy'], exp_names, top_exp).set_index('Experiment').drop('wt. Mean ± SD', axis=1).rank(method='dense') - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " Experiment &  Accuracy &  PR-AUC &  F1 \\\\\n",
      "\\midrule\n",
      "     BC-CNN &        18 &      15 &  15 \\\\\n",
      "     RANDOM &         0 &       0 &   7 \\\\\n",
      "  C-CNN (C) &        17 &      24 &  17 \\\\\n",
      "  F-CNN (C) &        19 &      27 &  24 \\\\\n",
      " FP-CNN (C) &        25 &      20 &  20 \\\\\n",
      " FL-CNN (C) &        20 &      15 &  14 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acc = pd.DataFrame(columns=['Accuracy'], data=(rE.prepare_top_experiments(results_s['accuracy'], exp_names, top_exp).set_index('Experiment').drop('wt. Mean ± SD', axis=1).rank(method='dense') - 1).sum(axis=1))\n",
    "df_pr = pd.DataFrame(columns=['PR-AUC'], data=(rE.prepare_top_experiments(results_s['pr'], exp_names, top_exp).set_index('Experiment').drop('wt. Mean ± SD', axis=1).rank(method='dense') - 1).sum(axis=1))\n",
    "df_f1 = pd.DataFrame(columns=['F1'], data=(rE.prepare_top_experiments(results_s['f1_score'], exp_names, top_exp).set_index('Experiment').drop('wt. Mean ± SD', axis=1).rank(method='dense') - 1).sum(axis=1))\n",
    "print(pd.concat((df_acc, df_pr, df_f1), axis=1).astype(int).reset_index().to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Metrics Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 132\n",
    "experiment = '{} - Seed {}'.format(seed, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Concatenate results and prepare for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results if necessary\n",
    "# for seed in range(123, 133):\n",
    "#     experiment = '{} - Seed {}'.format(seed, seed)\n",
    "#     experiment_folder = os.path.join(RESULTS, experiment)\n",
    "#     plotting_folder = os.path.join(experiment_folder, 'Plotting')\n",
    "#     rE.concat_validation_metrics(experiment_folder)\n",
    "#     for file in os.listdir(plotting_folder):\n",
    "#         old = os.path.join(plotting_folder, file)\n",
    "#         date, _, file = file.split('_')\n",
    "#         new = os.path.join(plotting_folder, file)\n",
    "#         os.rename(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top experiments\n",
    "top_exper = {'2-sessions-Centralized-pre-training.xlsx': 'C-CNN (C)', \n",
    "           '4-sessions-Federated-central-pre-training.xlsx': 'F-CNN (C)', \n",
    "           '7-sessions-Federated-central-pre-training-personalization.xlsx': 'FP-CNN (C)',\n",
    "           '10-sessions-Federated-central-pre-training-local-models.xlsx': 'FL-CNN (C)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PLT Parameters\n",
    "plt.rcParams.update({'font.size': 19, \n",
    "                     'font.family' : 'cmr10', \n",
    "                     'font.weight' : 'normal',\n",
    "                     'axes.titlesize' : 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plotting_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e5a959b1d71f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Find and sort file paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplotting_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplotting_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_exper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Plotting/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plotting_folder' is not defined"
     ]
    }
   ],
   "source": [
    "# Find and sort file paths\n",
    "file_paths = [os.path.join(plotting_folder, file) for file in sorted(os.listdir(plotting_folder)) if file in top_exper]\n",
    "order = [int(file_path.split('Plotting/')[1].split('-')[0]) for file_path in file_paths]\n",
    "file_paths = [path for _, path in sorted(zip(order, file_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in top_exper.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for seed in range(123, 133):\n",
    "        experiment = '{} - Seed {}'.format(seed, seed)\n",
    "        experiment_folder = os.path.join(RESULTS, experiment)\n",
    "        plotting_folder = os.path.join(experiment_folder, 'Plotting')\n",
    "        for file in os.listdir(plotting_folder):\n",
    "            if file == key:\n",
    "                df = pd.concat((df, pd.read_excel(os.path.join(plotting_folder, file))), sort=False)\n",
    "    df.groupby(['Session', 'Epoch']).mean().reset_index().to_excel(os.path.join(RESULTS, '999 - Sd Summary', 'MEAN_' + key))\n",
    "    df.groupby(['Session', 'Epoch']).std().reset_index().to_excel(os.path.join(RESULTS, '999 - Sd Summary', 'STD_' + key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort summary file paths\n",
    "mean_files = [os.path.join(RESULTS, '999 - Sd Summary', file) for file in os.listdir(os.path.join(RESULTS, '999 - Sd Summary')) if 'MEAN' in file]\n",
    "sd_files = [os.path.join(RESULTS, '999 - Sd Summary', file) for file in os.listdir(os.path.join(RESULTS, '999 - Sd Summary')) if 'STD' in file]\n",
    "order_mean = [int(file_path.split('MEAN_')[1].split('-')[0]) for file_path in mean_files]\n",
    "order_sd = [int(file_path.split('STD_')[1].split('-')[0]) for file_path in sd_files]\n",
    "mean_files = [path for _, path in sorted(zip(order_mean, mean_files))]\n",
    "sd_files = [path for _, path in sorted(zip(order_sd, sd_files))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy: Train / Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-f00b036145a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_simple_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_exper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFIGURES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trainingaccuracy_{}.pdf'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_paths' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = create_simple_grid(file_paths, np.arange(0,1.1,0.2), 'accuracy', top_exper)\n",
    "fig.savefig(os.path.join(FIGURES, 'trainingaccuracy_{}.pdf'.format(seed)), bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss: Train / Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = create_simple_grid(file_paths, np.arange(0,2.1,0.2), 'loss', top_exper)\n",
    "fig.savefig(os.path.join(FIGURES, 'trainingloss_{}.pdf'.format(seed)), bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AVG Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss: Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = create_avg_grid(mean_files, sd_files, np.arange(0,2.1,0.2), 'loss', top_exper)\n",
    "fig.savefig(os.path.join(FIGURES, 'trainingloss_MEAN.pdf'.format(seed)), bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy: Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = create_avg_grid(mean_files, sd_files, np.arange(0,1.1,0.2), 'accuracy', top_exper)\n",
    "fig.savefig(os.path.join(FIGURES, 'trainingaccuracy_MEAN.pdf'.format(seed)), bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share of Pain Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PLT Parameters\n",
    "plt.rcParams.update({'font.size': 18, \n",
    "                     'font.family' : 'cmr10', \n",
    "                     'font.weight' : 'normal',\n",
    "                     'axes.titlesize' : 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = draw_pain_bar_chart('Session')\n",
    "fig.savefig(os.path.join(FIGURES, 'painbar.pdf'), bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = draw_pain_bar_chart('Person')\n",
    "fig.savefig(os.path.join(FIGURES, 'painbar_person.pdf'), bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = {}\n",
    "model_types = ['PAIN_{}'.format(i) for i in [2, 4, 7, 10]]\n",
    "sessions = [6]\n",
    "subjects = [52, 80, 96]\n",
    "for model_type in model_types:\n",
    "    for session in sessions:\n",
    "        for subject in subjects:\n",
    "            conf_matrix[(model_type, session, subject)] = create_avg_conf_matrix(model_type, session, subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in model_types:\n",
    "    print()\n",
    "    print(model_type)\n",
    "    print(conf_matrix[(model_type, 6, 80)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FederatedLearning",
   "language": "python",
   "name": "federatedlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
