{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation\n",
    "A notebook that aids the preparation of a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, accuracy_score, precision_score, recall_score\n",
    "models = tf.keras.models  # like 'from tensorflow.keras import models' (PyCharm import issue workaround)\n",
    "layers = tf.keras.layers  # like 'from tensorflow.keras import layers' (PyCharm import issue workaround)\n",
    "optimizers = tf.keras.optimizers  # like 'from tensorflow.keras import optimizers' (PyCharm import issue workaround)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from Scripts import Data_Loader_Functions as Data_Loader\n",
    "from Scripts import Centralized_Pain_CNN as painCNN\n",
    "from Scripts.Experiments import experiment_federated_pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n",
      "1000 images processed\n",
      "2000 images processed\n",
      "3000 images processed\n",
      "4000 images processed\n",
      "5000 images processed\n",
      "6000 images processed\n",
      "7000 images processed\n",
      "8000 images processed\n",
      "9000 images processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 00:14:20.144496 4775114176 deprecation.py:323] From /Users/nico/PycharmProjects/FederatedLearning/venv/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py:1792: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n",
      "1000 images processed\n",
      "2000 images processed\n",
      "3000 images processed\n",
      "4000 images processed\n",
      "5000 images processed\n",
      "6000 images processed\n",
      "7000 images processed\n",
      "8000 images processed\n",
      "9000 images processed\n",
      "10000 images processed\n",
      "11000 images processed\n",
      "12000 images processed\n",
      "13000 images processed\n",
      "14000 images processed\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# train_path = os.path.join(cNN.ROOT, \"Data\", \"Augmented Data\", \"Pain Two-Step Augmentation\", \"group_1\")\n",
    "test_path = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain Two-Step Augmentation\", \"group_2_test\")\n",
    "train_path_add_data = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain Two-Step Augmentation\",\n",
    "                                   \"group_2_train\")\n",
    "test_data, test_labels = Data_Loader.load_pain_data(test_path)\n",
    "\n",
    "# Define labels for training\n",
    "person = 0\n",
    "label = 4  # Labels: [person, session, culture, frame, pain, Trans_1, Trans_2]\n",
    "\n",
    "# Prepare labels for training and evaluation\n",
    "# train_labels_ord = train_labels[:, label].astype(np.int)\n",
    "# train_labels_bin = Data_Loader.reduce_pain_label_categories(train_labels_ord, max_pain=1)\n",
    "test_labels_ord = test_labels[:, label].astype(np.int)\n",
    "test_labels_bin = Data_Loader.reduce_pain_label_categories(test_labels_ord, max_pain=1)\n",
    "# test_labels_people = test_labels[:, person].astype(np.int)\n",
    "\n",
    "# Load Pretrained model\n",
    "model = tf.keras.models.load_model('/Users/nico/PycharmProjects/FederatedLearning/Models/Pain/Centralized/'\n",
    "                                   '2019-07-23-051453_Centralized_PAIN_Centralized-Training.h5')\n",
    "\n",
    "# Load additional data\n",
    "add_train_data, add_train_labels = Data_Loader.load_pain_data(train_path_add_data)\n",
    "add_train_labels_ord = add_train_labels[:, label].astype(np.int)\n",
    "train_labels_bin = Data_Loader.reduce_pain_label_categories(add_train_labels_ord, max_pain=1)\n",
    "add_test_labels_people = add_train_labels[:, person].astype(np.int)\n",
    "\n",
    "# Split Data into shards\n",
    "split = 6\n",
    "add_train_data = np.array_split(add_train_data, split)\n",
    "train_labels_bin = np.array_split(train_labels_bin, split)\n",
    "add_test_labels_people = np.array_split(add_test_labels_people, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'TEST'\n",
    "clients = 10\n",
    "dataset = 'PAIN'\n",
    "rounds = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "-------------------------------------------------------- Communication Round 1 --------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------- Client 0 --------------------------------------------------------------\n",
      "Train on 247 samples\n",
      "247/247 [==============================] - 2s 9ms/sample - loss: 0.7270 - accuracy: 0.5020\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------- Client 1 --------------------------------------------------------------\n",
      "Train on 247 samples\n",
      "247/247 [==============================] - 2s 8ms/sample - loss: 0.7039 - accuracy: 0.5344\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------- Client 2 --------------------------------------------------------------\n",
      "Train on 246 samples\n",
      "246/246 [==============================] - 2s 9ms/sample - loss: 0.6933 - accuracy: 0.5366\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------- Client 3 --------------------------------------------------------------\n",
      "Train on 247 samples\n",
      "247/247 [==============================] - 2s 9ms/sample - loss: 0.7141 - accuracy: 0.5425\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------- Client 4 --------------------------------------------------------------\n",
      "Train on 246 samples\n",
      "246/246 [==============================] - 2s 9ms/sample - loss: 0.7275 - accuracy: 0.5122\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------- Client 5 --------------------------------------------------------------\n",
      "Train on 248 samples\n",
      "248/248 [==============================] - 2s 9ms/sample - loss: 0.6948 - accuracy: 0.6008\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------- Client 6 --------------------------------------------------------------\n",
      "Train on 246 samples\n",
      "246/246 [==============================] - 2s 9ms/sample - loss: 0.6977 - accuracy: 0.5610\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------- Client 7 --------------------------------------------------------------\n",
      "Train on 246 samples\n",
      "246/246 [==============================] - 2s 9ms/sample - loss: 0.7203 - accuracy: 0.5203\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------- Client 8 --------------------------------------------------------------\n",
      "Train on 247 samples\n",
      "247/247 [==============================] - 2s 9ms/sample - loss: 0.6876 - accuracy: 0.5385\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------- Client 9 --------------------------------------------------------------\n",
      "Train on 247 samples\n",
      "247/247 [==============================] - 2s 10ms/sample - loss: 0.6893 - accuracy: 0.5587\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4a890e95bb9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mexperiment_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_shard-{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     experiment_federated_pain(clients, dataset, experiment_new, data, labels, test_data, test_labels_bin, rounds,\n\u001b[0;32m----> 6\u001b[0;31m                                   people=people)\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mshard_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/FederatedLearning/Scripts/Experiments.py\u001b[0m in \u001b[0;36mexperiment_federated_pain\u001b[0;34m(clients, dataset, experiment, train_data, train_labels, test_data, test_labels, rounds, epochs, split, participants, people)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                                              \u001b[0mnum_participating_clients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparticipants\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                                              \u001b[0mpeople\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpeople\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                                              )\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/FederatedLearning/Scripts/Federated_Transfer_Learning_CNN.py\u001b[0m in \u001b[0;36mfederated_learning\u001b[0;34m(communication_rounds, num_of_clients, train_data, train_labels, test_data, test_labels, epochs, num_participating_clients, people)\u001b[0m\n\u001b[1;32m    195\u001b[0m         communication_round(model, num_of_clients, train_data, train_labels, epochs, weights_accountant,\n\u001b[1;32m    196\u001b[0m                             num_participating_clients)\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_federated_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomm_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_accountant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_accountant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/FederatedLearning/Scripts/Federated_Transfer_Learning_CNN.py\u001b[0m in \u001b[0;36mevaluate_federated_cnn\u001b[0;34m(test_data, test_labels, comm_round, model, weights_accountant, history, people)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpainCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_pain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomm_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/FederatedLearning/Scripts/Centralized_Pain_CNN.py\u001b[0m in \u001b[0;36mevaluate_pain_cnn\u001b[0;34m(model, epoch, test_data, test_labels, history, people)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Else just compute aggregate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeople\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_individual_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/FederatedLearning/Scripts/Centralized_Pain_CNN.py\u001b[0m in \u001b[0;36mcompute_individual_metrics\u001b[0;34m(epoch, loss, people, test_labels, y_pred, predictions)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_individual_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpeople\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Person'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Y_Pred'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Y_True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "# Train on additional shards and evaluate performance\n",
    "shard_counter = 1\n",
    "for data, labels, people in zip(add_train_data, train_labels_bin, add_test_labels_people):\n",
    "    experiment_new = experiment + \"_shard-{}\".format(shard_counter)\n",
    "    experiment_federated_pain(clients, dataset, experiment_new, data, labels, test_data, test_labels_bin, rounds,\n",
    "                                  people=people)\n",
    "    shard_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0]),\n",
       " array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "        27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "        44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
       "        61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n",
       "        78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94,\n",
       "        95, 96, 97, 98, 99])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = [int(0.01 * len(x)), int(0.1 * len(x))]\n",
    "np.array_split(x, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_shards(data, labels, split):\n",
    "    \"\"\"\n",
    "    Utility function, splitting data into specified subsets of shards. Scales the split array to 100%.\n",
    "\n",
    "    :param data:                numpy arrray\n",
    "    :param labels:              numpy array\n",
    "    :param split:               list of percentage split points, e.g. [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "                                final point serves for scaling only, i.e. no array will be split after that point\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    split = [int(x / max(split) * len(data)) for x in split][:-1]\n",
    "    data = np.array_split(data, split)\n",
    "    labels = np.array_split(labels, split)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumconc(array):\n",
    "    total = np.concatenate(array)\n",
    "    return np.array([*map(total.__getitem__, map(slice, np.fromiter(map(len, array), int, len(array)).cumsum()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(100)*1.1\n",
    "labels = np.arange(100,200)\n",
    "split = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = split_data_into_shards(data, labels, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0.]), array([0. , 1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7]),\n",
       "       array([ 0. ,  1.1,  2.2,  3.3,  4.4,  5.5,  6.6,  7.7,  8.8,  9.9, 11. ,\n",
       "       12.1, 13.2, 14.3, 15.4, 16.5]),\n",
       "       array([ 0. ,  1.1,  2.2,  3.3,  4.4,  5.5,  6.6,  7.7,  8.8,  9.9, 11. ,\n",
       "       12.1, 13.2, 14.3, 15.4, 16.5, 17.6, 18.7, 19.8, 20.9, 22. , 23.1,\n",
       "       24.2, 25.3, 26.4, 27.5, 28.6, 29.7, 30.8, 31.9, 33. , 34.1, 35.2]),\n",
       "       array([ 0. ,  1.1,  2.2,  3.3,  4.4,  5.5,  6.6,  7.7,  8.8,  9.9, 11. ,\n",
       "       12.1, 13.2, 14.3, 15.4, 16.5, 17.6, 18.7, 19.8, 20.9, 22. , 23.1,\n",
       "       24.2, 25.3, 26.4, 27.5, 28.6, 29.7, 30.8, 31.9, 33. , 34.1, 35.2,\n",
       "       36.3, 37.4, 38.5, 39.6, 40.7, 41.8, 42.9, 44. , 45.1, 46.2, 47.3,\n",
       "       48.4, 49.5, 50.6, 51.7, 52.8, 53.9]),\n",
       "       array([ 0. ,  1.1,  2.2,  3.3,  4.4,  5.5,  6.6,  7.7,  8.8,  9.9, 11. ,\n",
       "       12.1, 13.2, 14.3, 15.4, 16.5, 17.6, 18.7, 19.8, 20.9, 22. , 23.1,\n",
       "       24.2, 25.3, 26.4, 27.5, 28.6, 29.7, 30.8, 31.9, 33. , 34.1, 35.2,\n",
       "       36.3, 37.4, 38.5, 39.6, 40.7, 41.8, 42.9, 44. , 45.1, 46.2, 47.3,\n",
       "       48.4, 49.5, 50.6, 51.7, 52.8, 53.9, 55. , 56.1, 57.2, 58.3, 59.4,\n",
       "       60.5, 61.6, 62.7, 63.8, 64.9, 66. , 67.1, 68.2, 69.3, 70.4, 71.5]),\n",
       "       array([ 0. ,  1.1,  2.2,  3.3,  4.4,  5.5,  6.6,  7.7,  8.8,  9.9, 11. ,\n",
       "       12.1, 13.2, 14.3, 15.4, 16.5, 17.6, 18.7, 19.8, 20.9, 22. , 23.1,\n",
       "       24.2, 25.3, 26.4, 27.5, 28.6, 29.7, 30.8, 31.9, 33. , 34.1, 35.2,\n",
       "       36.3, 37.4, 38.5, 39.6, 40.7, 41.8, 42.9, 44. , 45.1, 46.2, 47.3,\n",
       "       48.4, 49.5, 50.6, 51.7, 52.8, 53.9, 55. , 56.1, 57.2, 58.3, 59.4,\n",
       "       60.5, 61.6, 62.7, 63.8, 64.9, 66. , 67.1, 68.2, 69.3, 70.4, 71.5,\n",
       "       72.6, 73.7, 74.8, 75.9, 77. , 78.1, 79.2, 80.3, 81.4, 82.5, 83.6,\n",
       "       84.7, 85.8, 86.9, 88. , 89.1, 90.2]),\n",
       "       array([  0. ,   1.1,   2.2,   3.3,   4.4,   5.5,   6.6,   7.7,   8.8,\n",
       "         9.9,  11. ,  12.1,  13.2,  14.3,  15.4,  16.5,  17.6,  18.7,\n",
       "        19.8,  20.9,  22. ,  23.1,  24.2,  25.3,  26.4,  27.5,  28.6,\n",
       "        29.7,  30.8,  31.9,  33. ,  34.1,  35.2,  36.3,  37.4,  38.5,\n",
       "        39.6,  40.7,  41.8,  42.9,  44. ,  45.1,  46.2,  47.3,  48.4,\n",
       "        49.5,  50.6,  51.7,  52.8,  53.9,  55. ,  56.1,  57.2,  58.3,\n",
       "        59.4,  60.5,  61.6,  62.7,  63.8,  64.9,  66. ,  67.1,  68.2,\n",
       "        69.3,  70.4,  71.5,  72.6,  73.7,  74.8,  75.9,  77. ,  78.1,\n",
       "        79.2,  80.3,  81.4,  82.5,  83.6,  84.7,  85.8,  86.9,  88. ,\n",
       "        89.1,  90.2,  91.3,  92.4,  93.5,  94.6,  95.7,  96.8,  97.9,\n",
       "        99. , 100.1, 101.2, 102.3, 103.4, 104.5, 105.6, 106.7, 107.8,\n",
       "       108.9])], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FederatedLearning",
   "language": "python",
   "name": "federatedlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
