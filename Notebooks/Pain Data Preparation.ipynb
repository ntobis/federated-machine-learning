{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pain Data Preparation\n",
    "This notebook prepares the pain dataset in to be able to successfully train a convolutional neural network. Data augmentation techniques such as greyscaling, histogram equalization, etc. are employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from Scripts import Data_Loader_Functions as DL\n",
    "from Scripts import Image_Processor as IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder paths\n",
    "RAW_DATA = os.path.join(module_path, \"Data\", \"Raw Data\", \"Pain\")\n",
    "AUGMENTED_DATA = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain\")\n",
    "AUGMENTED_DATA_TWOSTEP = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain Two-Step Augmentation\")\n",
    "AUGMENTED_DATA_FLEXIBLE = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Flexible Augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DL.create_pain_df(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14604"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['Session'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12332"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['Pain'] == '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original pain distribution\n",
    "img_paths = np.array(DL.get_image_paths(RAW_DATA))\n",
    "labels = np.array(DL.get_labels(img_paths))\n",
    "no_pain_labels = labels[labels[:,4].astype(int)==0]\n",
    "pain_labels = labels[labels[:,4].astype(int)>0]\n",
    "print(\"Pain Labels:\", len(pain_labels))\n",
    "print(\"No Pain Labels:\", len(no_pain_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of clients per group\n",
    "g1_img_paths = [x for x in os.listdir(os.path.join(RAW_DATA, \"group_1\")) if x != '.DS_Store']\n",
    "g2_img_paths = [x for x in os.listdir(os.path.join(RAW_DATA, \"group_2\")) if x != '.DS_Store']\n",
    "print(\"Group 1:\", len(g1_img_paths))\n",
    "print(\"Group 2:\", len(g2_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of sessions per client\n",
    "g1_img_paths = np.array(DL.get_image_paths(os.path.join(RAW_DATA, \"group_1\")))\n",
    "g2_img_paths = np.array(DL.get_image_paths(os.path.join(RAW_DATA, \"group_2\")))\n",
    "g1_labels = np.array(DL.get_labels(g1_img_paths))\n",
    "g2_labels = np.array(DL.get_labels(g2_img_paths))\n",
    "df_1 = pd.DataFrame(g1_labels, columns=['Person','Session','Culture','Frame','Pain']).astype(int)\n",
    "df_2 = pd.DataFrame(g2_labels, columns=['Person','Session','Culture','Frame','Pain']).astype(int)\n",
    "df_1['Group'] = 1\n",
    "df_2['Group'] = 2\n",
    "df = pd.concat([df_1, df_2])\n",
    "sess_num = pd.DataFrame(df.groupby(['Person', 'Group'])['Session'].nunique()).sort_values(['Group','Person'])\n",
    "sess_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of sessions per group\n",
    "print(\"Average Sessions Group 1: {0:.2f}\".format(df_1.groupby('Person')['Session'].nunique().mean()))\n",
    "print(\"Average Sessions Group 2: {0:.2f}\".format(df_2.groupby('Person')['Session'].nunique().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pain / No Pain per group\n",
    "print(\"Group 1 Pain/No Pain/Ratio: {} | {}\".format(df[(df['Group'] == 1) & (df['Pain'] == 0)].count()[0], df[(df['Group'] == 1) & (df['Pain'] > 0)].count()[0]))\n",
    "print(\"Group 2 Pain/No Pain/Ratio: {} | {}\".format(df[(df['Group'] == 2) & (df['Pain'] == 0)].count()[0], df[(df['Group'] == 2) & (df['Pain'] > 0)].count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Images\n",
    "We will now process the images. Preprocessing includes converting to greyscale, and histogram equalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "IP.bulk_process_images(RAW_DATA, PREPROCESSED_DATA, \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip images and copy originals into augmented data folder\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_TWOSTEP, \".jpg\", \"flip\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_TWOSTEP, \".jpg\", \"original\", \"pain\", label_threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate Originals and flipped images, and ensure that naming conventions stay consistent\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_flipped.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_original.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_rename_files(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_rotated\", \"_straight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop images to same maximum width and height (10-degree rotation in previous step cropped rotated images \n",
    "# down to (215, 215), so this is chosen as a max width/height)\n",
    "IP.bulk_crop_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, (215, 215))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample augmented data\n",
    "DL.downsample_data(os.path.join(AUGMENTED_DATA_TWOSTEP, \"group_1\"))\n",
    "DL.downsample_data(os.path.join(AUGMENTED_DATA_TWOSTEP, \"group_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible Data Augmentation\n",
    "Code snippets that allow to move doubly augmented data around quickly. Main purpose is to ensure that the same images in original or augmented form are not being used for training and testing at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Double-augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirror folder structure\n",
    "print(\"Mirror Folder Structure\")\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA_FLEXIBLE)\n",
    "\n",
    "# Flip images and copy originals into augmented data folder\n",
    "print(\"Flip Images\")\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_FLEXIBLE, \".jpg\", \"flip\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_FLEXIBLE, \".jpg\", \"original\", \"pain\", label_threshold=-1)\n",
    "\n",
    "# Rotate Originals and flipped images, and ensure that naming conventions stay consistent\n",
    "print(\"Rotate Images\")\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_flipped.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_original.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_rename_files(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_rotated\", \"_straight\")\n",
    "\n",
    "# Crop images to same maximum width and height (10-degree rotation in previous step cropped rotated images \n",
    "# down to (215, 215), so this is chosen as a max width/height)\n",
    "print(\"Crop Images\")\n",
    "IP.bulk_crop_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, (215, 215), \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reset Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving all images into the \"raw\" subfolder\n",
    "DL.reset_to_raw(AUGMENTED_DATA_FLEXIBLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting all empty folders\n",
    "DL.delete_empty_folders(AUGMENTED_DATA_FLEXIBLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image paths and corresponding labels into a dataframe\n",
    "img_paths = np.array(DL.get_image_paths(AUGMENTED_DATA_FLEXIBLE))\n",
    "labels = np.array(DL.get_labels(img_paths))\n",
    "df = pd.DataFrame(labels, columns=['Person','Session','Culture','Frame','Pain', 'Trans_1', 'Trans_2'])\n",
    "df[['Person','Session','Culture','Frame','Pain']] = df[['Person','Session','Culture','Frame','Pain']].astype(int)\n",
    "df['img_path'] = img_paths\n",
    "df[['Trans_1', 'Trans_2', 'img_path']] = df[['Trans_1', 'Trans_2', 'img_path']].astype(str)\n",
    "df = df.sort_values(['Person', 'Session', 'Frame', 'Trans_1', 'Trans_2'], ascending=[True, True, True, False, False]).reset_index(drop=True)\n",
    "df['temp_id'] = df['Person'].astype(str) + df['Session'].astype(str) + df['Frame'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Remove Subject 101 from the data\n",
    "Subject 101 only has negative examples \"0\" and will therefore show \"0%\" on metrics like \"Recall\" or \"Precision\", skewing output graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pain Labels Subject 101 :  0\n"
     ]
    }
   ],
   "source": [
    "# Proving that subject 101 only has 0 labels\n",
    "subject = 101\n",
    "print(\"# Pain Labels Subject {} : \".format(subject), np.sum(df[df['Person'] == subject]['Pain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing subject 101 from the data\n",
    "df = df[df['Person'] != 101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Redistribute Data for Training\n",
    "Do one of the subsection steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution checking\n",
    "def print_distribution(df_train, df_test):\n",
    "    print(\"\\033[1mTrain\\t\\t\\t\\t   |Test\\033[0m\")\n",
    "    for train, test in zip(df_train.groupby('Person'), df_test.groupby('Person')):\n",
    "        print(\"Subject {} Train:\\t{}\\t{:.0%}|{:.0%}  Subject {} Test:\\t{}\"\n",
    "              .format(train[0], len(train[1]), len(train[1]) / (len(train[1]) + len(test[1])),\n",
    "                      len(test[1]) / (len(train[1]) + len(test[1])), test[0], len(test[1])))\n",
    "    print(\"-\" * 68)\n",
    "    print(\"Total Original Train:\\t{}\\t{:.0%}|{:.0%}  Total Original Test:\\t{}\"\n",
    "          .format(len(df_train), len(df_train) / (len(df_train) + len(df_test)),\n",
    "                  len(df_test) / (len(df_train) + len(df_test)), len(df_test)))\n",
    "\n",
    "def print_pain_distribution(df_train, df_test):\n",
    "    print(\"Train:          {:.0%} |\".format(len(df_train) / (len(df_test) + len(df_train))),\n",
    "          \"Test:          {:.0%}\".format(len(df_test) / (len(df_test) + len(df_train))), )\n",
    "    print(\"Train No Pain: {} |\".format(len(df_train[df_train['Pain'] == 0])),\n",
    "          \"Test No Pain: {}\".format(len(df_test[df_test['Pain'] == 0])))\n",
    "    print(\"Train Pain:    {} |\".format(len(df_train[df_train['Pain'] > 0])),\n",
    "          \"Test Pain:    {}\".format(len(df_test[df_test['Pain'] > 0])))\n",
    "    print(\"Train Total:  {} |\".format(len(df_train)), \"Test Total:   {}\".format(len(df_test)))\n",
    "    print()\n",
    "    print(\"Total:        {}\".format(len(df_train) + len(df_test)))\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Duplicates:\", sum(df_train['temp_id'].isin(df_test['temp_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into two groups\n",
    "group_1 = [42, 47, 49, 66, 95, 97, 103, 106, 108, 121, 123, 124]\n",
    "df_1 = df[df['Person'].isin(group_1)]\n",
    "df_2 = df[~df['Person'].isin(group_1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Redistribute Naively\n",
    "In this step, the we will just downsample the data and put it into two groups, without accounting for potential duplicates in test and train data (e.g. \"original\" in train, and \"flipped\" in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1))\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample second group\n",
    "df_2_pain_1 = df_2[df_2['Pain'] > 0]\n",
    "df_2_pain_0 = df_2[df_2['Pain'] == 0].sample(len(df_2_pain_1))\n",
    "df_2_downsampled = pd.concat((df_2_pain_0, df_2_pain_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Redistribute - No Mutation Duplicates\n",
    "In this step the data is split so that the same image in a mutated form is not in train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1))\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Pain Frames into Train and Test 60 / 40\n",
    "ratio = 0.6\n",
    "\n",
    "temp_ids_pain = df_2[df_2['Pain'] > 0]['temp_id'].unique()\n",
    "temp_ids_pain_train = np.random.choice(temp_ids_pain, int(ratio * len(temp_ids_pain)), replace=False)\n",
    "temp_ids_pain_test = temp_ids_pain[np.isin(temp_ids_pain, temp_ids_pain_train) == False]\n",
    "df_2_pain_train = df_2[df_2['temp_id'].isin(temp_ids_pain_train)]\n",
    "df_2_pain_test = df_2[df_2['temp_id'].isin(temp_ids_pain_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Pain Frames into Train and Test 60 / 40, with the same number of Train / Test Samples as Pain\n",
    "temp_ids_no_pain = df_2[df_2['Pain'] == 0]['temp_id'].unique()\n",
    "temp_ids_no_pain_train = np.random.choice(temp_ids_no_pain, len(df_2_pain_train), replace=False)\n",
    "temp_ids_no_pain_test = np.random.choice(temp_ids_no_pain[np.isin(temp_ids_no_pain, temp_ids_no_pain_train) == False], len(df_2_pain_test), replace=False)\n",
    "df_2_pain_0_train = df_2[df_2['temp_id'].isin(temp_ids_no_pain_train)].sample(len(df_2_pain_train))\n",
    "df_2_pain_0_test = df_2[df_2['temp_id'].isin(temp_ids_no_pain_test)].sample(len(df_2_pain_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test\n",
    "df_2_train = pd.concat((df_2_pain_train, df_2_pain_0_train))\n",
    "df_2_test = pd.concat((df_2_pain_test, df_2_pain_0_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that everything went well\n",
    "print_pain_distribution(df_2_train, df_2_test)\n",
    "print()\n",
    "print_distribution(df_2_train, df_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3: Redistribute - No Test Augmentation\n",
    "In this step we redistribute the data so that only the train dataset is balanced. The test dataset maintains its original composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1))\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_originals = df_2[(df_2['Trans_1'] == 'original') & (df_2['Trans_2'] == 'straight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split original images into train and test, on a per person basis, 60/40\n",
    "ratio = 0.6\n",
    "\n",
    "df_2_originals_train = pd.DataFrame(columns=df_2_originals.columns)\n",
    "df_2_originals_test = pd.DataFrame(columns=df_2_originals.columns)\n",
    "for df_person in df_2_originals.groupby('Person'):\n",
    "    df_person_train = df_person[1].sample(frac=ratio)\n",
    "    df_person_test = df_person[1].drop(df_person_train.index)\n",
    "    df_2_originals_train = pd.concat((df_2_originals_train, df_person_train))\n",
    "    df_2_originals_test = pd.concat((df_2_originals_test, df_person_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that everything went well in this first stage. Expected output is a 60/40 split for every subject,\n",
    "# as well as significantly more 'no pain' than 'pain' subjects\n",
    "print_pain_distribution(df_2_originals_train, df_2_originals_test)\n",
    "print(\"\\n--------------------------------------------------------------------\\n\")\n",
    "print_distribution(df_2_originals_train, df_2_originals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_train_ids = df_2_originals_train['temp_id'].unique()\n",
    "df_2_train = df_2[df_2['temp_id'].isin(df_2_train_ids)]\n",
    "df_2_train_pain = df_2_train[df_2_train['Pain'] > 0]\n",
    "df_2_train_no_pain = df_2_train[df_2_train['Pain'] == 0].sample(len(df_2_train_pain))\n",
    "df_2_train = pd.concat((df_2_train_pain, df_2_train_no_pain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_test = df_2_originals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final distribution with augmented train images\n",
    "print_pain_distribution(df_2_train, df_2_test)\n",
    "print(\"\\n--------------------------------------------------------------------\\n\")\n",
    "print_distribution(df_2_train, df_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.4: Original Distribution\n",
    "In this step, the data used is only the original images that have been preprocessed but not augmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original images\n",
    "df_1_original = df_1[(df_1['Trans_1'] == 'original') & (df_1['Trans_2'] == 'straight')]\n",
    "df_2_original = df_2[(df_2['Trans_1'] == 'original') & (df_2['Trans_2'] == 'straight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_2 into train and test on a per person basis\n",
    "ratio = 0.6\n",
    "\n",
    "df_2_originals_train = pd.DataFrame(columns=df_2_original.columns)\n",
    "df_2_originals_test = pd.DataFrame(columns=df_2_original.columns)\n",
    "for df_person in df_2_original.groupby('Person'):\n",
    "    df_person_train = df_person[1].sample(frac=ratio)\n",
    "    df_person_test = df_person[1].drop(df_person_train.index)\n",
    "    df_2_originals_train = pd.concat((df_2_originals_train, df_person_train))\n",
    "    df_2_originals_test = pd.concat((df_2_originals_test, df_person_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_train = df_2_originals_train\n",
    "df_2_test = df_2_originals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Group 1 Distribution\n",
    "print(\"Group 1 Pain:    \", np.sum(df_1_original['Pain'] > 0),\"\\t|  {:.0%}\".format(np.sum(df_1_original['Pain'] > 0) / len(df_1_original)))\n",
    "print(\"Group 1 No Pain: \", np.sum(df_1_original['Pain'] == 0),\"|  {:.0%}\".format(np.sum(df_1_original['Pain'] == 0) / len(df_1_original)))\n",
    "print(\"\\n\\t\\tPain \\t| No Pain\\tPain\\t| No Pain\")\n",
    "print(\"-\"*35,\"-\"*25)\n",
    "for df_person in df_1_original.groupby('Person'):\n",
    "    print(\"Person {}:\\t{}\\t| {}\\t\\t{:.0%}\\t|  {:.0%}\".format(df_person[0], np.sum(df_person[1]['Pain'] > 0), np.sum(df_person[1]['Pain'] == 0), np.sum(df_person[1]['Pain'] > 0) / len(df_person[1]), np.sum(df_person[1]['Pain'] == 0) / len(df_person[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final distribution with augmented train images\n",
    "print_pain_distribution(df_2_originals_train, df_2_originals_test)\n",
    "print(\"\\n--------------------------------------------------------------------\\n\")\n",
    "print_distribution(df_2_originals_train, df_2_originals_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.4: Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1))\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into sessions\n",
    "session_dfs_2 = np.array([idx_df for idx_df in df_2.groupby('Session')])\n",
    "session_paths = [os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_2\", \"session_\" + str(sess)) for sess in session_dfs_2[:,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Allocate Groups\n",
    "Move the image files into the correct folders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_group(df, path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    for f_path in df['img_path'].values:\n",
    "        os.rename(f_path, os.path.join(path, os.path.basename(f_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate Group 1\n",
    "group_1_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_1\")\n",
    "allocate_group(df_1_downsampled, group_1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1: Allocate group 2 into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate Group 2 Train / Test\n",
    "train_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_train')\n",
    "test_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_test')\n",
    "\n",
    "allocate_group(df_2_train, train_path)\n",
    "allocate_group(df_2_test, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Success, expected outcome is no instances of pain images in the \"Raw\" folder, a large group one,\n",
    "# and smaller group 2 train and test\n",
    "print(\"Group 1:        {}\".format(len(os.listdir(group_1_path))))\n",
    "print(\"Group 2 Train:  {}\".format(len(os.listdir(train_path))))\n",
    "print(\"Group 2 Test:   {}\".format(len(os.listdir(test_path))))\n",
    "print(\"Raw:            {}\".format(len(os.listdir(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'raw')))))\n",
    "print(\"Raw Pain Img's: {}\".format(np.sum(np.minimum(np.array(DL.get_labels(DL.get_image_paths(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'raw'))))[:,4].astype(int), 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2: Allocation group 2 into sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate into sessions\n",
    "if not os.path.isdir(os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_2\")):\n",
    "    os.mkdir(os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_2\"))\n",
    "for df, path in zip(session_dfs_2[:,1], session_paths):\n",
    "    allocate_group(df, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Success\n",
    "for path in sorted(session_paths):\n",
    "    print(\"Session {}:\".format(path[-1]), len(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0.6, 0.4], [1, 0], [1, 0], [0, 1]])\n",
    "b = np.array([[0.6, 0.4], [0.4, 0.6], [0.3, 0.7], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.argmax(b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.argmax(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected dimension in the range [-1, 1), but got 1\n\t [[node ArgMax_12 (defined at <ipython-input-50-b38330ac7b0b>:20) ]]\n\nCaused by op 'ArgMax_12', defined at:\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-57-75e75c91ff9b>\", line 2, in <module>\n    print(FN(c, d).eval())\n  File \"<ipython-input-50-b38330ac7b0b>\", line 20, in FN\n    y_pred = tf.argmax(y_pred, 1)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 86, in argmax\n    return argmax_v2(input, axis, output_type, name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 115, in argmax_v2\n    return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 879, in arg_max\n    name=name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Expected dimension in the range [-1, 1), but got 1\n\t [[node ArgMax_12 (defined at <ipython-input-50-b38330ac7b0b>:20) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected dimension in the range [-1, 1), but got 1\n\t [[{{node ArgMax_12}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-75e75c91ff9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \"\"\"\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5179\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5180\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5181\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected dimension in the range [-1, 1), but got 1\n\t [[node ArgMax_12 (defined at <ipython-input-50-b38330ac7b0b>:20) ]]\n\nCaused by op 'ArgMax_12', defined at:\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-57-75e75c91ff9b>\", line 2, in <module>\n    print(FN(c, d).eval())\n  File \"<ipython-input-50-b38330ac7b0b>\", line 20, in FN\n    y_pred = tf.argmax(y_pred, 1)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 86, in argmax\n    return argmax_v2(input, axis, output_type, name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 115, in argmax_v2\n    return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 879, in arg_max\n    name=name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Expected dimension in the range [-1, 1), but got 1\n\t [[node ArgMax_12 (defined at <ipython-input-50-b38330ac7b0b>:20) ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "    print(FN(c, d).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TP(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, 1)\n",
    "    y_true = tf.argmax(y_true, 1)\n",
    "    return tf.math.count_nonzero(y_pred * y_true)\n",
    "\n",
    "\n",
    "def FP(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, 1)\n",
    "    y_true = tf.argmax(y_true, 1)\n",
    "    return tf.math.count_nonzero(y_pred * (y_true - 1))\n",
    "\n",
    "\n",
    "def TN(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, 1)\n",
    "    y_true = tf.argmax(y_true, 1)\n",
    "    return tf.math.count_nonzero((y_pred - 1) * (y_true - 1))\n",
    "\n",
    "\n",
    "def FN(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, 1)\n",
    "    y_true = tf.argmax(y_true, 1)\n",
    "    return tf.math.count_nonzero((y_pred - 1) * y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'count_nonzero_12/Sum:0' shape=() dtype=int64>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
