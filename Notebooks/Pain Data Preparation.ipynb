{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pain Data Preparation\n",
    "This notebook prepares the pain dataset in to be able to successfully train a convolutional neural network. Data augmentation techniques such as greyscaling, histogram equalization, etc. are employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from Scripts import Data_Loader_Functions as DL\n",
    "from Scripts import Image_Processor as IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder paths\n",
    "RAW_DATA = os.path.join(module_path, \"Data\", \"Raw Data\", \"Pain\")\n",
    "AUGMENTED_DATA = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain\")\n",
    "AUGMENTED_DATA_TWOSTEP = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain Two-Step Augmentation\")\n",
    "AUGMENTED_DATA_FLEXIBLE = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Flexible Augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folder Structure\n",
    "First, we will duplicate the folder structure in \"Raw Data\" into \"Preprocessed Data\" and \"Augmented Data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate folder structure\n",
    "DL.mirror_folder_structure(RAW_DATA, PREPROCESSED_DATA)\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA)\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA_TWOSTEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original pain distribution\n",
    "img_paths = np.array(DL.get_image_paths(RAW_DATA))\n",
    "labels = np.array(DL.get_labels(img_paths))\n",
    "no_pain_labels = labels[labels[:,4].astype(int)==0]\n",
    "pain_labels = labels[labels[:,4].astype(int)>0]\n",
    "print(\"Pain Labels:\", len(pain_labels))\n",
    "print(\"No Pain Labels:\", len(no_pain_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of clients per group\n",
    "g1_img_paths = [x for x in os.listdir(os.path.join(RAW_DATA, \"group_1\")) if x != '.DS_Store']\n",
    "g2_img_paths = [x for x in os.listdir(os.path.join(RAW_DATA, \"group_2\")) if x != '.DS_Store']\n",
    "print(\"Group 1:\", len(g1_img_paths))\n",
    "print(\"Group 2:\", len(g2_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of sessions per client\n",
    "g1_img_paths = np.array(DL.get_image_paths(os.path.join(RAW_DATA, \"group_1\")))\n",
    "g2_img_paths = np.array(DL.get_image_paths(os.path.join(RAW_DATA, \"group_2\")))\n",
    "g1_labels = np.array(DL.get_labels(g1_img_paths))\n",
    "g2_labels = np.array(DL.get_labels(g2_img_paths))\n",
    "df_1 = pd.DataFrame(g1_labels, columns=['Person','Session','Culture','Frame','Pain']).astype(int)\n",
    "df_2 = pd.DataFrame(g2_labels, columns=['Person','Session','Culture','Frame','Pain']).astype(int)\n",
    "df_1['Group'] = 1\n",
    "df_2['Group'] = 2\n",
    "df = pd.concat([df_1, df_2])\n",
    "sess_num = pd.DataFrame(df.groupby(['Person', 'Group'])['Session'].nunique()).sort_values(['Group','Person'])\n",
    "sess_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of sessions per group\n",
    "print(\"Average Sessions Group 1: {0:.2f}\".format(df_1.groupby('Person')['Session'].nunique().mean()))\n",
    "print(\"Average Sessions Group 2: {0:.2f}\".format(df_2.groupby('Person')['Session'].nunique().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pain / No Pain per group\n",
    "print(\"Group 1 Pain/No Pain/Ratio: {} | {}\".format(df[(df['Group'] == 1) & (df['Pain'] == 0)].count()[0], df[(df['Group'] == 1) & (df['Pain'] > 0)].count()[0]))\n",
    "print(\"Group 2 Pain/No Pain/Ratio: {} | {}\".format(df[(df['Group'] == 2) & (df['Pain'] == 0)].count()[0], df[(df['Group'] == 2) & (df['Pain'] > 0)].count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Images\n",
    "We will now process the images. Preprocessing includes converting to greyscale, and histogram equalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "IP.bulk_process_images(RAW_DATA, PREPROCESSED_DATA, \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip images and copy originals into augmented data folder\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_TWOSTEP, \".jpg\", \"flip\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_TWOSTEP, \".jpg\", \"original\", \"pain\", label_threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate Originals and flipped images, and ensure that naming conventions stay consistent\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_flipped.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_original.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_rename_files(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_rotated\", \"_straight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop images to same maximum width and height (10-degree rotation in previous step cropped rotated images \n",
    "# down to (215, 215), so this is chosen as a max width/height)\n",
    "IP.bulk_crop_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, (215, 215))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample augmented data\n",
    "DL.downsample_data(os.path.join(AUGMENTED_DATA_TWOSTEP, \"group_1\"))\n",
    "DL.downsample_data(os.path.join(AUGMENTED_DATA_TWOSTEP, \"group_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Group 2 Data\n",
    "Split Group 2 Data into 40% Test Data and an additional 60% Test Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame to enable filtering by labels\n",
    "img_paths = DL.get_image_paths(os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2'))\n",
    "labels = DL.get_labels(img_paths)\n",
    "df = pd.DataFrame(labels, columns=['Person','Session','Culture','Frame','Pain', 'Trans_1', 'Trans_2'])\n",
    "df[['Person','Session','Culture','Frame','Pain']] = df[['Person','Session','Culture','Frame','Pain']].astype(int)\n",
    "df['img_path'] = img_paths\n",
    "df[['Trans_1', 'Trans_2', 'img_path']] = df[['Trans_1', 'Trans_2', 'img_path']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset into Train and Test\n",
    "origin_path = os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2')\n",
    "train_path = os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2_train')\n",
    "test_path = os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2_test')\n",
    "DL.move_train_test_data(df, origin_path, train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that split is 40 / 60\n",
    "img_train = DL.get_image_paths(train_path)\n",
    "img_test = DL.get_image_paths(test_path)\n",
    "print(\"Test: {0:.2f}\".format(len(img_test) / (len(img_test) + len(img_train))))\n",
    "print(\"Train: {0:.2f}\".format(len(img_train) / (len(img_test) + len(img_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset (Randomly)\n",
    "Splitting the dataset into training data and test data, by sampling without replacement from the train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images and select 20% at random\n",
    "img_paths = DL.get_image_paths(AUGMENTED_DATA_TWOSTEP)\n",
    "np.random.shuffle(img_paths)\n",
    "split_idx = int(len(img_paths)*0.2)\n",
    "img_paths_test = img_paths[:split_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that each client is represented with ~20% in the test data set\n",
    "img_per_client_test = np.unique(np.array(DL.get_labels(img_paths_test))[:,0], return_counts=True)[1]\n",
    "img_per_client_total = np.unique(np.array(DL.get_labels(img_paths))[:,0], return_counts=True)[1]\n",
    "img_per_client_test / img_per_client_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the test set is balanced\n",
    "pain = np.array(img_paths_test)[np.array(DL.get_labels(img_paths_test))[:,4].astype(int) >= 1]\n",
    "pain_test_labels = np.array(DL.get_labels(pain))\n",
    "all_test_labels = np.array(DL.get_labels(img_paths_test))\n",
    "print(\"Test Pain Split:\",len(pain_labels) / len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the split for each client in the test set\n",
    "DL.print_pain_split_per_client(all_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move test data set into test folder\n",
    "for src in img_paths_test:\n",
    "    file = os.path.basename(src)\n",
    "    dest = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(src))), 'test')\n",
    "    if not os.path.isdir(dest):\n",
    "        os.mkdir(dest)\n",
    "    try:\n",
    "        os.rename(src, os.path.join(dest, file))\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible Data Augmentation\n",
    "Code snippets that allow to move doubly augmented data around quickly. Main purpose is to ensure that the same images in original or augmented form are not being used for training and testing at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Double-augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirror folder structure\n",
    "print(\"Mirror Folder Structure\")\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA_FLEXIBLE)\n",
    "\n",
    "# Flip images and copy originals into augmented data folder\n",
    "print(\"Flip Images\")\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_FLEXIBLE, \".jpg\", \"flip\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_FLEXIBLE, \".jpg\", \"original\", \"pain\", label_threshold=-1)\n",
    "\n",
    "# Rotate Originals and flipped images, and ensure that naming conventions stay consistent\n",
    "print(\"Rotate Images\")\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_flipped.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_original.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_rename_files(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_rotated\", \"_straight\")\n",
    "\n",
    "# Crop images to same maximum width and height (10-degree rotation in previous step cropped rotated images \n",
    "# down to (215, 215), so this is chosen as a max width/height)\n",
    "print(\"Crop Images\")\n",
    "IP.bulk_crop_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, (215, 215), \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reset Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving all images into the \"raw\" subfolder\n",
    "DL.reset_to_raw(AUGMENTED_DATA_FLEXIBLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting all empty folders\n",
    "DL.delete_empty_folders(AUGMENTED_DATA_FLEXIBLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image paths and corresponding labels into a dataframe\n",
    "img_paths = np.array(DL.get_image_paths(AUGMENTED_DATA_FLEXIBLE))\n",
    "labels = np.array(DL.get_labels(img_paths))\n",
    "df = pd.DataFrame(labels, columns=['Person','Session','Culture','Frame','Pain', 'Trans_1', 'Trans_2'])\n",
    "df[['Person','Session','Culture','Frame','Pain']] = df[['Person','Session','Culture','Frame','Pain']].astype(int)\n",
    "df['img_path'] = img_paths\n",
    "df[['Trans_1', 'Trans_2', 'img_path']] = df[['Trans_1', 'Trans_2', 'img_path']].astype(str)\n",
    "df = df.sort_values(['Person', 'Session', 'Frame', 'Trans_1', 'Trans_2'], ascending=[True, True, True, False, False]).reset_index(drop=True)\n",
    "df['temp_id'] = df['Person'].astype(str) + df['Session'].astype(str) + df['Frame'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Remove Subject 101 from the data\n",
    "Subject 101 only has negative examples \"0\" and will therefore show \"0%\" on metrics like \"Recall\" or \"Precision\", skewing output graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pain Labels Subject 101 :  0\n"
     ]
    }
   ],
   "source": [
    "# Proving that subject 101 only has 0 labels\n",
    "subject = 101\n",
    "print(\"# Pain Labels Subject {} : \".format(subject), np.sum(df[df['Person'] == subject]['Pain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing subject 101 from the data\n",
    "df = df[df['Person'] != 101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Redistribute Data for Training\n",
    "Do one of the subsection steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution checking\n",
    "def print_distribution(df_train, df_test):\n",
    "    print(\"\\033[1mTrain\\t\\t\\t\\t   |Test\\033[0m\")\n",
    "    for train, test in zip(df_train.groupby('Person'), df_test.groupby('Person')):\n",
    "        print(\"Subject {} Train:\\t{}\\t{:.0%}|{:.0%}  Subject {} Test:\\t{}\"\n",
    "              .format(train[0], len(train[1]), len(train[1]) / (len(train[1]) + len(test[1])),\n",
    "                      len(test[1]) / (len(train[1]) + len(test[1])), test[0], len(test[1])))\n",
    "    print(\"-\" * 68)\n",
    "    print(\"Total Original Train:\\t{}\\t{:.0%}|{:.0%}  Total Original Test:\\t{}\"\n",
    "          .format(len(df_train), len(df_train) / (len(df_train) + len(df_test)),\n",
    "                  len(df_test) / (len(df_train) + len(df_test)), len(df_test)))\n",
    "\n",
    "def print_pain_distribution(df_train, df_test):\n",
    "    print(\"Train:          {:.0%} |\".format(len(df_train) / (len(df_test) + len(df_train))),\n",
    "          \"Test:          {:.0%}\".format(len(df_test) / (len(df_test) + len(df_train))), )\n",
    "    print(\"Train No Pain: {} |\".format(len(df_train[df_train['Pain'] == 0])),\n",
    "          \"Test No Pain: {}\".format(len(df_test[df_test['Pain'] == 0])))\n",
    "    print(\"Train Pain:    {} |\".format(len(df_train[df_train['Pain'] > 0])),\n",
    "          \"Test Pain:    {}\".format(len(df_test[df_test['Pain'] > 0])))\n",
    "    print(\"Train Total:  {} |\".format(len(df_train)), \"Test Total:   {}\".format(len(df_test)))\n",
    "    print()\n",
    "    print(\"Total:        {}\".format(len(df_train) + len(df_test)))\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Duplicates:\", sum(df_train['temp_id'].isin(df_test['temp_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into two groups\n",
    "group_1 = [42, 47, 49, 66, 95, 97, 103, 106, 108, 121, 123, 124]\n",
    "df_1 = df[df['Person'].isin(group_1)]\n",
    "df_2 = df[df['Person'].isin(group_1) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Redistribute Naively\n",
    "In this step, the we will just downsample the data and put it into two groups, without accounting for potential duplicates in test and train data (e.g. \"original\" in train, and \"flipped\" in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1), random_state=123)\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample second group\n",
    "df_2_pain_1 = df_2[df_2['Pain'] > 0]\n",
    "df_2_pain_0 = df_2[df_2['Pain'] == 0].sample(len(df_2_pain_1), random_state=123)\n",
    "df_2_downsampled = pd.concat((df_2_pain_0, df_2_pain_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Redistribute - No Mutation Duplicates\n",
    "In this step the data is split so that the same image in a mutated form is not in train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1), random_state=123)\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Pain Frames into Train and Test 60 / 40\n",
    "np.random.seed(123)\n",
    "ratio = 0.6\n",
    "\n",
    "temp_ids_pain = df_2[df_2['Pain'] > 0]['temp_id'].unique()\n",
    "temp_ids_pain_train = np.random.choice(temp_ids_pain, int(ratio * len(temp_ids_pain)), replace=False)\n",
    "temp_ids_pain_test = temp_ids_pain[np.isin(temp_ids_pain, temp_ids_pain_train) == False]\n",
    "df_2_pain_train = df_2[df_2['temp_id'].isin(temp_ids_pain_train)]\n",
    "df_2_pain_test = df_2[df_2['temp_id'].isin(temp_ids_pain_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Pain Frames into Train and Test 60 / 40, with the same number of Train / Test Samples as Pain\n",
    "temp_ids_no_pain = df_2[df_2['Pain'] == 0]['temp_id'].unique()\n",
    "temp_ids_no_pain_train = np.random.choice(temp_ids_no_pain, len(df_2_pain_train), replace=False)\n",
    "temp_ids_no_pain_test = np.random.choice(temp_ids_no_pain[np.isin(temp_ids_no_pain, temp_ids_no_pain_train) == False], len(df_2_pain_test), replace=False)\n",
    "df_2_pain_0_train = df_2[df_2['temp_id'].isin(temp_ids_no_pain_train)].sample(len(df_2_pain_train))\n",
    "df_2_pain_0_test = df_2[df_2['temp_id'].isin(temp_ids_no_pain_test)].sample(len(df_2_pain_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test\n",
    "df_2_train = pd.concat((df_2_pain_train, df_2_pain_0_train))\n",
    "df_2_test = pd.concat((df_2_pain_test, df_2_pain_0_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:          60% | Test:          40%\n",
      "Train No Pain: 7396 | Test No Pain: 4936\n",
      "Train Pain:    7396 | Test Pain:    4936\n",
      "Train Total:  14792 | Test Total:   9872\n",
      "\n",
      "Total:        24664\n",
      "----------------------------------------\n",
      "Duplicates: 0\n",
      "\n",
      "\u001b[1mTrain\t\t\t\t   |Test\u001b[0m\n",
      "Subject 43 Train:\t665\t58%|42%  Subject 43 Test:\t482\n",
      "Subject 48 Train:\t512\t56%|44%  Subject 48 Test:\t398\n",
      "Subject 52 Train:\t1393\t60%|40%  Subject 52 Test:\t911\n",
      "Subject 59 Train:\t632\t61%|39%  Subject 59 Test:\t412\n",
      "Subject 64 Train:\t1000\t59%|41%  Subject 64 Test:\t683\n",
      "Subject 80 Train:\t2993\t61%|39%  Subject 80 Test:\t1952\n",
      "Subject 92 Train:\t1611\t60%|40%  Subject 92 Test:\t1068\n",
      "Subject 96 Train:\t1429\t62%|38%  Subject 96 Test:\t881\n",
      "Subject 107 Train:\t1717\t58%|42%  Subject 107 Test:\t1260\n",
      "Subject 109 Train:\t1234\t61%|39%  Subject 109 Test:\t788\n",
      "Subject 115 Train:\t768\t60%|40%  Subject 115 Test:\t522\n",
      "Subject 120 Train:\t838\t62%|38%  Subject 120 Test:\t515\n",
      "--------------------------------------------------------------------\n",
      "Total Original Train:\t14792\t60%|40%  Total Original Test:\t9872\n"
     ]
    }
   ],
   "source": [
    "# Verify that everything went well\n",
    "print_pain_distribution(df_2_train, df_2_test)\n",
    "print()\n",
    "print_distribution(df_2_train, df_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3: Redistribute - No Test Augmentation\n",
    "In this step we redistribute the data so that only the train dataset is balanced. The test dataset maintains its original composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1), random_state=123)\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_originals = df_2[(df_2['Trans_1'] == 'original') & (df_2['Trans_2'] == 'straight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split original images into train and test, on a per person basis, 60/40\n",
    "ratio = 0.6\n",
    "\n",
    "df_2_originals_train = pd.DataFrame(columns=df_2_originals.columns)\n",
    "df_2_originals_test = pd.DataFrame(columns=df_2_originals.columns)\n",
    "for df_person in df_2_originals.groupby('Person'):\n",
    "    df_person_train = df_person[1].sample(frac=ratio, random_state=123)\n",
    "    df_person_test = df_person[1].drop(df_person_train.index)\n",
    "    df_2_originals_train = pd.concat((df_2_originals_train, df_person_train))\n",
    "    df_2_originals_test = pd.concat((df_2_originals_test, df_person_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that everything went well in this first stage. Expected output is a 60/40 split for every subject,\n",
    "# as well as significantly more 'no pain' than 'pain' subjects\n",
    "print_pain_distribution(df_2_originals_train, df_2_originals_test)\n",
    "print(\"\\n--------------------------------------------------------------------\\n\")\n",
    "print_distribution(df_2_originals_train, df_2_originals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_train_ids = df_2_originals_train['temp_id'].unique()\n",
    "df_2_train = df_2[df_2['temp_id'].isin(df_2_train_ids)]\n",
    "df_2_train_pain = df_2_train[df_2_train['Pain'] > 0]\n",
    "df_2_train_no_pain = df_2_train[df_2_train['Pain'] == 0].sample(len(df_2_train_pain))\n",
    "df_2_train = pd.concat((df_2_train_pain, df_2_train_no_pain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final distribution with augmented train images\n",
    "print_pain_distribution(df_2_train, df_2_originals_test)\n",
    "print(\"\\n--------------------------------------------------------------------\\n\")\n",
    "print_distribution(df_2_train, df_2_originals_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.4: Original Distribution\n",
    "In this step, the data used is only the original images that have been preprocessed but not augmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original images\n",
    "df_1_original = df_1[(df_1['Trans_1'] == 'original') & (df_1['Trans_2'] == 'straight')]\n",
    "df_2_original = df_2[(df_2['Trans_1'] == 'original') & (df_2['Trans_2'] == 'straight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_2 into train and test on a per person basis\n",
    "ratio = 0.6\n",
    "\n",
    "df_2_originals_train = pd.DataFrame(columns=df_2_original.columns)\n",
    "df_2_originals_test = pd.DataFrame(columns=df_2_original.columns)\n",
    "for df_person in df_2_original.groupby('Person'):\n",
    "    df_person_train = df_person[1].sample(frac=ratio, random_state=123)\n",
    "    df_person_test = df_person[1].drop(df_person_train.index)\n",
    "    df_2_originals_train = pd.concat((df_2_originals_train, df_person_train))\n",
    "    df_2_originals_test = pd.concat((df_2_originals_test, df_person_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Group 1 Distribution\n",
    "print(\"Group 1 Pain:    \", np.sum(df_1_original['Pain'] > 0),\"\\t|  {:.0%}\".format(np.sum(df_1_original['Pain'] > 0) / len(df_1_original)))\n",
    "print(\"Group 1 No Pain: \", np.sum(df_1_original['Pain'] == 0),\"|  {:.0%}\".format(np.sum(df_1_original['Pain'] == 0) / len(df_1_original)))\n",
    "print(\"\\n\\t\\tPain \\t| No Pain\\tPain\\t| No Pain\")\n",
    "print(\"-\"*35,\"-\"*25)\n",
    "for df_person in df_1_original.groupby('Person'):\n",
    "    print(\"Person {}:\\t{}\\t| {}\\t\\t{:.0%}\\t|  {:.0%}\".format(df_person[0], np.sum(df_person[1]['Pain'] > 0), np.sum(df_person[1]['Pain'] == 0), np.sum(df_person[1]['Pain'] > 0) / len(df_person[1]), np.sum(df_person[1]['Pain'] == 0) / len(df_person[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final distribution with augmented train images\n",
    "print_pain_distribution(df_2_originals_train, df_2_originals_test)\n",
    "print(\"\\n--------------------------------------------------------------------\\n\")\n",
    "print_distribution(df_2_originals_train, df_2_originals_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.5: Session Data\n",
    "Split Data into sessions, for temporal subsequent learing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mGroup 1\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1ea0ec5e166b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print Session Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\033[1mGroup 1\\033[0m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdf_person\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Person'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Person {}:\\tImages: {}\\t Sessions: {}\\tImages/Session: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_person\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_person\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_person\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Session'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_person\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_person\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Session'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_1' is not defined"
     ]
    }
   ],
   "source": [
    "# Print Session Data\n",
    "print(\"\\033[1mGroup 1\\033[0m\")\n",
    "for df_person in df_1.groupby('Person'):\n",
    "    print(\"Person {}:\\tImages: {}\\t Sessions: {}\\tImages/Session: {}\".format(df_person[0], len(df_person[1]), len(pd.unique(df_person[1]['Session'])), round(len(df_person[1])/ len(pd.unique(df_person[1]['Session'])))))\n",
    "\n",
    "print(\"\\n\\033[1mGroup 2\\033[0m\")\n",
    "for df_person in df_2.groupby('Person'):\n",
    "    print(\"Person {}:\\tImages: {}\\t Sessions: {}\\tImages/Session: {}\".format(df_person[0], len(df_person[1]), len(pd.unique(df_person[1]['Session'])), round(len(df_person[1])/ len(pd.unique(df_person[1]['Session'])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Allocate Groups\n",
    "Move the image files into the correct folders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_group(df, path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    for f_path in df['img_path'].values:\n",
    "        os.rename(f_path, os.path.join(path, os.path.basename(f_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate Group 1\n",
    "group_1_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_1\")\n",
    "allocate_group(df_1_downsampled, group_1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate Group 2 Train / Test\n",
    "train_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_train')\n",
    "test_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_test')\n",
    "\n",
    "allocate_group(df_2_train, train_path)\n",
    "allocate_group(df_2_test, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1:        41416\n",
      "Group 2 Train:  14792\n",
      "Group 2 Test:   9872\n",
      "Raw:            126344\n",
      "Raw Pain Img's: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify Success, expected outcome is no instances of pain images in the \"Raw\" folder, a large group one,\n",
    "# and smaller group 2 train and test\n",
    "print(\"Group 1:        {}\".format(len(os.listdir(group_1_path))))\n",
    "print(\"Group 2 Train:  {}\".format(len(os.listdir(train_path))))\n",
    "print(\"Group 2 Test:   {}\".format(len(os.listdir(test_path))))\n",
    "print(\"Raw:            {}\".format(len(os.listdir(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'raw')))))\n",
    "print(\"Raw Pain Img's: {}\".format(np.sum(np.minimum(np.array(DL.get_labels(DL.get_image_paths(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'raw'))))[:,4].astype(int), 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n",
      "1000 images processed\n",
      "2000 images processed\n",
      "3000 images processed\n",
      "4000 images processed\n",
      "5000 images processed\n",
      "6000 images processed\n",
      "7000 images processed\n",
      "8000 images processed\n",
      "9000 images processed\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = DL.load_pain_data(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
