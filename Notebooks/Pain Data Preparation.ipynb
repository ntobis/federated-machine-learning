{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pain Data Preparation\n",
    "This notebook prepares the pain dataset in to be able to successfully train a convolutional neural network. Data augmentation techniques such as greyscaling, histogram equalization, etc. are employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from Scripts import Data_Loader_Functions as DL\n",
    "from Scripts import Image_Processor as IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder paths\n",
    "RAW_DATA = os.path.join(module_path, \"Data\", \"Raw Data\", \"Pain\")\n",
    "AUGMENTED_DATA = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain\")\n",
    "AUGMENTED_DATA_TWOSTEP = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain Two-Step Augmentation\")\n",
    "AUGMENTED_DATA_FLEXIBLE = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Flexible Augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folder Structure\n",
    "First, we will duplicate the folder structure in \"Raw Data\" into \"Preprocessed Data\" and \"Augmented Data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate folder structure\n",
    "DL.mirror_folder_structure(RAW_DATA, PREPROCESSED_DATA)\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA)\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA_TWOSTEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original pain distribution\n",
    "img_paths = np.array(DL.get_image_paths(RAW_DATA))\n",
    "labels = np.array(DL.get_labels(img_paths))\n",
    "no_pain_labels = labels[labels[:,4].astype(int)==0]\n",
    "pain_labels = labels[labels[:,4].astype(int)>0]\n",
    "print(\"Pain Labels:\", len(pain_labels))\n",
    "print(\"No Pain Labels:\", len(no_pain_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of clients per group\n",
    "g1_img_paths = [x for x in os.listdir(os.path.join(RAW_DATA, \"group_1\")) if x != '.DS_Store']\n",
    "g2_img_paths = [x for x in os.listdir(os.path.join(RAW_DATA, \"group_2\")) if x != '.DS_Store']\n",
    "print(\"Group 1:\", len(g1_img_paths))\n",
    "print(\"Group 2:\", len(g2_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of sessions per client\n",
    "g1_img_paths = np.array(DL.get_image_paths(os.path.join(RAW_DATA, \"group_1\")))\n",
    "g2_img_paths = np.array(DL.get_image_paths(os.path.join(RAW_DATA, \"group_2\")))\n",
    "g1_labels = np.array(DL.get_labels(g1_img_paths))\n",
    "g2_labels = np.array(DL.get_labels(g2_img_paths))\n",
    "df_1 = pd.DataFrame(g1_labels, columns=['Person','Session','Culture','Frame','Pain']).astype(int)\n",
    "df_2 = pd.DataFrame(g2_labels, columns=['Person','Session','Culture','Frame','Pain']).astype(int)\n",
    "df_1['Group'] = 1\n",
    "df_2['Group'] = 2\n",
    "df = pd.concat([df_1, df_2])\n",
    "sess_num = pd.DataFrame(df.groupby(['Person', 'Group'])['Session'].nunique()).sort_values(['Group','Person'])\n",
    "sess_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of sessions per group\n",
    "print(\"Average Sessions Group 1: {0:.2f}\".format(df_1.groupby('Person')['Session'].nunique().mean()))\n",
    "print(\"Average Sessions Group 2: {0:.2f}\".format(df_2.groupby('Person')['Session'].nunique().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pain / No Pain per group\n",
    "print(\"Group 1 Pain/No Pain/Ratio: {} | {}\".format(df[(df['Group'] == 1) & (df['Pain'] == 0)].count()[0], df[(df['Group'] == 1) & (df['Pain'] > 0)].count()[0]))\n",
    "print(\"Group 2 Pain/No Pain/Ratio: {} | {}\".format(df[(df['Group'] == 2) & (df['Pain'] == 0)].count()[0], df[(df['Group'] == 2) & (df['Pain'] > 0)].count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Images\n",
    "We will now process the images. Preprocessing includes converting to greyscale, and histogram equalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "IP.bulk_process_images(RAW_DATA, PREPROCESSED_DATA, \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip images and copy originals into augmented data folder\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_TWOSTEP, \".jpg\", \"flip\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_TWOSTEP, \".jpg\", \"original\", \"pain\", label_threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate Originals and flipped images, and ensure that naming conventions stay consistent\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_flipped.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_original.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_rename_files(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_rotated\", \"_straight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop images to same maximum width and height (10-degree rotation in previous step cropped rotated images \n",
    "# down to (215, 215), so this is chosen as a max width/height)\n",
    "IP.bulk_crop_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, (215, 215))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample augmented data\n",
    "DL.downsample_data(os.path.join(AUGMENTED_DATA_TWOSTEP, \"group_1\"))\n",
    "DL.downsample_data(os.path.join(AUGMENTED_DATA_TWOSTEP, \"group_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Group 2 Data\n",
    "Split Group 2 Data into 40% Test Data and an additional 60% Test Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame to enable filtering by labels\n",
    "img_paths = DL.get_image_paths(os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2'))\n",
    "labels = DL.get_labels(img_paths)\n",
    "df = pd.DataFrame(labels, columns=['Person','Session','Culture','Frame','Pain', 'Trans_1', 'Trans_2'])\n",
    "df[['Person','Session','Culture','Frame','Pain']] = df[['Person','Session','Culture','Frame','Pain']].astype(int)\n",
    "df['img_path'] = img_paths\n",
    "df[['Trans_1', 'Trans_2', 'img_path']] = df[['Trans_1', 'Trans_2', 'img_path']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset into Train and Test\n",
    "origin_path = os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2')\n",
    "train_path = os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2_train')\n",
    "test_path = os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2_test')\n",
    "DL.move_train_test_data(df, origin_path, train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that split is 40 / 60\n",
    "img_train = DL.get_image_paths(train_path)\n",
    "img_test = DL.get_image_paths(test_path)\n",
    "print(\"Test: {0:.2f}\".format(len(img_test) / (len(img_test) + len(img_train))))\n",
    "print(\"Train: {0:.2f}\".format(len(img_train) / (len(img_test) + len(img_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset (Randomly)\n",
    "Splitting the dataset into training data and test data, by sampling without replacement from the train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images and select 20% at random\n",
    "img_paths = DL.get_image_paths(AUGMENTED_DATA_TWOSTEP)\n",
    "np.random.shuffle(img_paths)\n",
    "split_idx = int(len(img_paths)*0.2)\n",
    "img_paths_test = img_paths[:split_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that each client is represented with ~20% in the test data set\n",
    "img_per_client_test = np.unique(np.array(DL.get_labels(img_paths_test))[:,0], return_counts=True)[1]\n",
    "img_per_client_total = np.unique(np.array(DL.get_labels(img_paths))[:,0], return_counts=True)[1]\n",
    "img_per_client_test / img_per_client_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the test set is balanced\n",
    "pain = np.array(img_paths_test)[np.array(DL.get_labels(img_paths_test))[:,4].astype(int) >= 1]\n",
    "pain_test_labels = np.array(DL.get_labels(pain))\n",
    "all_test_labels = np.array(DL.get_labels(img_paths_test))\n",
    "print(\"Test Pain Split:\",len(pain_labels) / len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the split for each client in the test set\n",
    "DL.print_pain_split_per_client(all_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move test data set into test folder\n",
    "for src in img_paths_test:\n",
    "    file = os.path.basename(src)\n",
    "    dest = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(src))), 'test')\n",
    "    if not os.path.isdir(dest):\n",
    "        os.mkdir(dest)\n",
    "    try:\n",
    "        os.rename(src, os.path.join(dest, file))\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible Data Augmentation\n",
    "Code snippets that allow to move doubly augmented data around quickly. Main purpose is to ensure that the same images in original or augmented form are not being used for training and testing at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Double-augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirror folder structure\n",
    "print(\"Mirror Folder Structure\")\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA_FLEXIBLE)\n",
    "\n",
    "# Flip images and copy originals into augmented data folder\n",
    "print(\"Flip Images\")\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_FLEXIBLE, \".jpg\", \"flip\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_FLEXIBLE, \".jpg\", \"original\", \"pain\", label_threshold=-1)\n",
    "\n",
    "# Rotate Originals and flipped images, and ensure that naming conventions stay consistent\n",
    "print(\"Rotate Images\")\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_flipped.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_original.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_rename_files(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_rotated\", \"_straight\")\n",
    "\n",
    "# Crop images to same maximum width and height (10-degree rotation in previous step cropped rotated images \n",
    "# down to (215, 215), so this is chosen as a max width/height)\n",
    "print(\"Crop Images\")\n",
    "IP.bulk_crop_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, (215, 215), \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reset Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving all images into the \"raw\" subfolder\n",
    "DL.reset_to_raw(AUGMENTED_DATA_FLEXIBLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting all empty folders\n",
    "DL.delete_empty_folders(AUGMENTED_DATA_FLEXIBLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image paths and corresponding labels into a dataframe\n",
    "img_paths = np.array(DL.get_image_paths(AUGMENTED_DATA_FLEXIBLE))\n",
    "labels = np.array(DL.get_labels(img_paths))\n",
    "df = pd.DataFrame(labels, columns=['Person','Session','Culture','Frame','Pain', 'Trans_1', 'Trans_2'])\n",
    "df[['Person','Session','Culture','Frame','Pain']] = df[['Person','Session','Culture','Frame','Pain']].astype(int)\n",
    "df['img_path'] = img_paths\n",
    "df[['Trans_1', 'Trans_2', 'img_path']] = df[['Trans_1', 'Trans_2', 'img_path']].astype(str)\n",
    "df = df.sort_values(['Person', 'Session', 'Frame', 'Trans_1', 'Trans_2'], ascending=[True, True, True, False, False]).reset_index(drop=True)\n",
    "df['temp_id'] = df['Person'].astype(str) + df['Session'].astype(str) + df['Frame'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Remove Subject 101 from the data\n",
    "Subject 101 only has negative examples \"0\" and will therefore show \"0%\" on metrics like \"Recall\" or \"Precision\", skewing output graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pain Labels Subject 101 :  0\n"
     ]
    }
   ],
   "source": [
    "# Proving that subject 101 only has 0 labels\n",
    "subject = 101\n",
    "print(\"# Pain Labels Subject {} : \".format(subject), np.sum(df[df['Person'] == subject]['Pain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing subject 101 from the data\n",
    "df = df[df['Person'] != 101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Redistribute Data for Training\n",
    "Do one of the subsection steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into two groups\n",
    "group_1 = [42, 47, 49, 66, 95, 97, 103, 106, 108, 121, 123, 124]\n",
    "df_1 = df[df['Person'].isin(group_1)]\n",
    "df_2 = df[df['Person'].isin(group_1) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1), random_state=123)\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Redistribute Naively\n",
    "In this step, the we will just downsample the data and put it into two groups, without accounting for potential duplicates in test and train data (e.g. \"original\" in train, and \"flipped\" in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample second group\n",
    "df_2_pain_1 = df_2[df_2['Pain'] > 0]\n",
    "df_2_pain_0 = df_2[df_2['Pain'] == 0].sample(len(df_2_pain_1), random_state=123)\n",
    "df_2_downsampled = pd.concat((df_2_pain_0, df_2_pain_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Redistribute - No Mutation Duplicates\n",
    "In this step the data is split so that the same image in a mutated form is not in train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Pain Frames into Train and Test 60 / 40\n",
    "np.random.seed(123)\n",
    "ratio = 0.6\n",
    "\n",
    "temp_ids_pain = df_2[df_2['Pain'] > 0]['temp_id'].unique()\n",
    "temp_ids_pain_train = np.random.choice(temp_ids_pain, int(ratio * len(temp_ids_pain)), replace=False)\n",
    "temp_ids_pain_test = temp_ids_pain[np.isin(temp_ids_pain, temp_ids_pain_train) == False]\n",
    "df_2_pain_train = df_2[df_2['temp_id'].isin(temp_ids_pain_train)]\n",
    "df_2_pain_test = df_2[df_2['temp_id'].isin(temp_ids_pain_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Pain Frames into Train and Test 60 / 40, with the same number of Train / Test Samples as Pain\n",
    "temp_ids_no_pain = df_2[df_2['Pain'] == 0]['temp_id'].unique()\n",
    "temp_ids_no_pain_train = np.random.choice(temp_ids_no_pain, len(df_2_pain_train), replace=False)\n",
    "temp_ids_no_pain_test = np.random.choice(temp_ids_no_pain[np.isin(temp_ids_no_pain, temp_ids_no_pain_train) == False], len(df_2_pain_test), replace=False)\n",
    "df_2_pain_0_train = df_2[df_2['temp_id'].isin(temp_ids_no_pain_train)].sample(len(df_2_pain_train))\n",
    "df_2_pain_0_test = df_2[df_2['temp_id'].isin(temp_ids_no_pain_test)].sample(len(df_2_pain_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test\n",
    "df_2_train = pd.concat((df_2_pain_train, df_2_pain_0_train))\n",
    "df_2_test = pd.concat((df_2_pain_test, df_2_pain_0_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:          60% | Test:          40%\n",
      "Train No Pain: 7396 | Test No Pain: 4936\n",
      "Train Pain:    7396 | Test Pain:    4936\n",
      "Train Total:  14792 | Test Total:   9872\n",
      "\n",
      "Total:        24664\n",
      "----------------------------------------\n",
      "Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify that everything went well\n",
    "print(\"Train:          {:.0%} |\".format(len(df_2_train) / (len(df_2_test) + len(df_2_train))),\n",
    "      \"Test:          {:.0%}\".format(len(df_2_test) / (len(df_2_test) + len(df_2_train))), )\n",
    "print(\"Train No Pain: {} |\".format(len(df_2_train[df_2_train['Pain'] == 0])), \"Test No Pain: {}\".format(len(df_2_test[df_2_test['Pain'] == 0])))\n",
    "print(\"Train Pain:    {} |\".format(len(df_2_train[df_2_train['Pain'] > 0])),  \"Test Pain:    {}\".format(len(df_2_test[df_2_test['Pain'] > 0])))\n",
    "print(\"Train Total:  {} |\".format(len(df_2_train)), \"Test Total:   {}\".format(len(df_2_test)))\n",
    "print()\n",
    "print(\"Total:        {}\".format(len(df_2_train) + len(df_2_test)))\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Duplicates:\", sum(df_2_train['temp_id'].isin(df_2_test['temp_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Redistribute - No Test Augmentation\n",
    "In this step we redistribute the data so that only the train dataset is balanced. The test dataset maintains its original composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_originals = df_2[(df_2['Trans_1'] == 'original') & (df_2['Trans_2'] == 'straight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "8536      43        0        0      0     0  original  straight   \n",
      "8540      43        0        0      1     0  original  straight   \n",
      "8544      43        0        0      2     0  original  straight   \n",
      "8548      43        0        0      3     0  original  straight   \n",
      "8552      43        0        0      4     0  original  straight   \n",
      "\n",
      "                                               img_path temp_id  \n",
      "8536  /Users/nico/PycharmProjects/FederatedLearning/...    4300  \n",
      "8540  /Users/nico/PycharmProjects/FederatedLearning/...    4301  \n",
      "8544  /Users/nico/PycharmProjects/FederatedLearning/...    4302  \n",
      "8548  /Users/nico/PycharmProjects/FederatedLearning/...    4303  \n",
      "8552  /Users/nico/PycharmProjects/FederatedLearning/...    4304  \n",
      "       Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "19448      48        0        0      0     0  original  straight   \n",
      "19452      48        0        0      1     0  original  straight   \n",
      "19456      48        0        0      2     0  original  straight   \n",
      "19460      48        0        0      3     0  original  straight   \n",
      "19464      48        0        0      4     0  original  straight   \n",
      "\n",
      "                                                img_path temp_id  \n",
      "19448  /Users/nico/PycharmProjects/FederatedLearning/...    4800  \n",
      "19452  /Users/nico/PycharmProjects/FederatedLearning/...    4801  \n",
      "19456  /Users/nico/PycharmProjects/FederatedLearning/...    4802  \n",
      "19460  /Users/nico/PycharmProjects/FederatedLearning/...    4803  \n",
      "19464  /Users/nico/PycharmProjects/FederatedLearning/...    4804  \n",
      "       Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "33848      52        0        0      0     0  original  straight   \n",
      "33852      52        0        0      1     0  original  straight   \n",
      "33856      52        0        0      2     0  original  straight   \n",
      "33860      52        0        0      3     0  original  straight   \n",
      "33864      52        0        0      4     0  original  straight   \n",
      "\n",
      "                                                img_path temp_id  \n",
      "33848  /Users/nico/PycharmProjects/FederatedLearning/...    5200  \n",
      "33852  /Users/nico/PycharmProjects/FederatedLearning/...    5201  \n",
      "33856  /Users/nico/PycharmProjects/FederatedLearning/...    5202  \n",
      "33860  /Users/nico/PycharmProjects/FederatedLearning/...    5203  \n",
      "33864  /Users/nico/PycharmProjects/FederatedLearning/...    5204  \n",
      "       Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "44284      59        0        0      0     0  original  straight   \n",
      "44288      59        0        0      1     0  original  straight   \n",
      "44292      59        0        0      2     0  original  straight   \n",
      "44296      59        0        0      3     0  original  straight   \n",
      "44300      59        0        0      4     0  original  straight   \n",
      "\n",
      "                                                img_path temp_id  \n",
      "44284  /Users/nico/PycharmProjects/FederatedLearning/...    5900  \n",
      "44288  /Users/nico/PycharmProjects/FederatedLearning/...    5901  \n",
      "44292  /Users/nico/PycharmProjects/FederatedLearning/...    5902  \n",
      "44296  /Users/nico/PycharmProjects/FederatedLearning/...    5903  \n",
      "44300  /Users/nico/PycharmProjects/FederatedLearning/...    5904  \n",
      "       Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "47376      64        0        0      0     0  original  straight   \n",
      "47380      64        0        0      1     0  original  straight   \n",
      "47384      64        0        0      2     0  original  straight   \n",
      "47388      64        0        0      3     0  original  straight   \n",
      "47392      64        0        0      4     0  original  straight   \n",
      "\n",
      "                                                img_path temp_id  \n",
      "47376  /Users/nico/PycharmProjects/FederatedLearning/...    6400  \n",
      "47380  /Users/nico/PycharmProjects/FederatedLearning/...    6401  \n",
      "47384  /Users/nico/PycharmProjects/FederatedLearning/...    6402  \n",
      "47388  /Users/nico/PycharmProjects/FederatedLearning/...    6403  \n",
      "47392  /Users/nico/PycharmProjects/FederatedLearning/...    6404  \n",
      "       Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "63408      80        0        0      0     0  original  straight   \n",
      "63412      80        0        0      1     0  original  straight   \n",
      "63416      80        0        0      2     0  original  straight   \n",
      "63420      80        0        0      3     0  original  straight   \n",
      "63424      80        0        0      4     0  original  straight   \n",
      "\n",
      "                                                img_path temp_id  \n",
      "63408  /Users/nico/PycharmProjects/FederatedLearning/...    8000  \n",
      "63412  /Users/nico/PycharmProjects/FederatedLearning/...    8001  \n",
      "63416  /Users/nico/PycharmProjects/FederatedLearning/...    8002  \n",
      "63420  /Users/nico/PycharmProjects/FederatedLearning/...    8003  \n",
      "63424  /Users/nico/PycharmProjects/FederatedLearning/...    8004  \n",
      "       Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "71264      92        0        0      0     3  original  straight   \n",
      "71268      92        0        0      1     3  original  straight   \n",
      "71272      92        0        0      2     3  original  straight   \n",
      "71276      92        0        0      3     3  original  straight   \n",
      "71280      92        0        0      4     3  original  straight   \n",
      "\n",
      "                                                img_path temp_id  \n",
      "71264  /Users/nico/PycharmProjects/FederatedLearning/...    9200  \n",
      "71268  /Users/nico/PycharmProjects/FederatedLearning/...    9201  \n",
      "71272  /Users/nico/PycharmProjects/FederatedLearning/...    9202  \n",
      "71276  /Users/nico/PycharmProjects/FederatedLearning/...    9203  \n",
      "71280  /Users/nico/PycharmProjects/FederatedLearning/...    9204  \n",
      "       Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "80480      96        0        0      0     0  original  straight   \n",
      "80484      96        0        0      1     0  original  straight   \n",
      "80488      96        0        0      2     0  original  straight   \n",
      "80492      96        0        0      3     0  original  straight   \n",
      "80496      96        0        0      4     0  original  straight   \n",
      "\n",
      "                                                img_path temp_id  \n",
      "80480  /Users/nico/PycharmProjects/FederatedLearning/...    9600  \n",
      "80484  /Users/nico/PycharmProjects/FederatedLearning/...    9601  \n",
      "80488  /Users/nico/PycharmProjects/FederatedLearning/...    9602  \n",
      "80492  /Users/nico/PycharmProjects/FederatedLearning/...    9603  \n",
      "80496  /Users/nico/PycharmProjects/FederatedLearning/...    9604  \n",
      "        Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "140036     107        0        0      0     0  original  straight   \n",
      "140040     107        0        0      1     0  original  straight   \n",
      "140044     107        0        0      2     0  original  straight   \n",
      "140048     107        0        0      3     0  original  straight   \n",
      "140052     107        0        0      4     0  original  straight   \n",
      "\n",
      "                                                 img_path temp_id  \n",
      "140036  /Users/nico/PycharmProjects/FederatedLearning/...   10700  \n",
      "140040  /Users/nico/PycharmProjects/FederatedLearning/...   10701  \n",
      "140044  /Users/nico/PycharmProjects/FederatedLearning/...   10702  \n",
      "140048  /Users/nico/PycharmProjects/FederatedLearning/...   10703  \n",
      "140052  /Users/nico/PycharmProjects/FederatedLearning/...   10704  \n",
      "        Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "159832     109        0        0      0     0  original  straight   \n",
      "159836     109        0        0      1     0  original  straight   \n",
      "159840     109        0        0      2     0  original  straight   \n",
      "159844     109        0        0      3     0  original  straight   \n",
      "159848     109        0        0      4     0  original  straight   \n",
      "\n",
      "                                                 img_path temp_id  \n",
      "159832  /Users/nico/PycharmProjects/FederatedLearning/...   10900  \n",
      "159836  /Users/nico/PycharmProjects/FederatedLearning/...   10901  \n",
      "159840  /Users/nico/PycharmProjects/FederatedLearning/...   10902  \n",
      "159844  /Users/nico/PycharmProjects/FederatedLearning/...   10903  \n",
      "159848  /Users/nico/PycharmProjects/FederatedLearning/...   10904  \n",
      "        Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "167444     115        0        0      0     0  original  straight   \n",
      "167448     115        0        0      1     0  original  straight   \n",
      "167452     115        0        0      2     0  original  straight   \n",
      "167456     115        0        0      3     0  original  straight   \n",
      "167460     115        0        0      4     0  original  straight   \n",
      "\n",
      "                                                 img_path temp_id  \n",
      "167444  /Users/nico/PycharmProjects/FederatedLearning/...   11500  \n",
      "167448  /Users/nico/PycharmProjects/FederatedLearning/...   11501  \n",
      "167452  /Users/nico/PycharmProjects/FederatedLearning/...   11502  \n",
      "167456  /Users/nico/PycharmProjects/FederatedLearning/...   11503  \n",
      "167460  /Users/nico/PycharmProjects/FederatedLearning/...   11504  \n",
      "        Person  Session  Culture  Frame  Pain   Trans_1   Trans_2  \\\n",
      "172576     120        0        0      0     0  original  straight   \n",
      "172580     120        0        0      1     0  original  straight   \n",
      "172584     120        0        0      2     0  original  straight   \n",
      "172588     120        0        0      3     0  original  straight   \n",
      "172592     120        0        0      4     0  original  straight   \n",
      "\n",
      "                                                 img_path temp_id  \n",
      "172576  /Users/nico/PycharmProjects/FederatedLearning/...   12000  \n",
      "172580  /Users/nico/PycharmProjects/FederatedLearning/...   12001  \n",
      "172584  /Users/nico/PycharmProjects/FederatedLearning/...   12002  \n",
      "172588  /Users/nico/PycharmProjects/FederatedLearning/...   12003  \n",
      "172592  /Users/nico/PycharmProjects/FederatedLearning/...   12004  \n"
     ]
    }
   ],
   "source": [
    "for person in df_2_originals.groupby('Person'):\n",
    "    print(person[1].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Allocate Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_group(df, path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    for f_path in df['img_path'].values:\n",
    "        os.rename(f_path, os.path.join(path, os.path.basename(f_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate Group 1\n",
    "group_1_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_1\")\n",
    "allocate_group(df_1_downsampled, group_1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate Group 2 Train / Test\n",
    "train_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_train')\n",
    "test_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_test')\n",
    "\n",
    "allocate_group(df_2_train, train_path)\n",
    "allocate_group(df_2_test, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1:        41416\n",
      "Group 2 Train:  14792\n",
      "Group 2 Test:   9872\n",
      "Raw:            126344\n",
      "Raw Pain Img's: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify Success\n",
    "print(\"Group 1:        {}\".format(len(os.listdir(group_1_path))))\n",
    "print(\"Group 2 Train:  {}\".format(len(os.listdir(train_path))))\n",
    "print(\"Group 2 Test:   {}\".format(len(os.listdir(test_path))))\n",
    "print(\"Raw:            {}\".format(len(os.listdir(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'raw')))))\n",
    "print(\"Raw Pain Img's: {}\".format(np.sum(np.minimum(np.array(DL.get_labels(DL.get_image_paths(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'raw'))))[:,4].astype(int), 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FederatedLearning",
   "language": "python",
   "name": "federatedlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
