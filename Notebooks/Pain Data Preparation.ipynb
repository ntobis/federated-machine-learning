{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pain Data Preparation\n",
    "This notebook prepares the pain dataset in to be able to successfully train a convolutional neural network. Data augmentation techniques such as greyscaling, histogram equalization, etc. are employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from Scripts import Data_Loader_Functions as DL\n",
    "from Scripts import Image_Processor as IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder paths\n",
    "RAW_DATA = os.path.join(module_path, \"Data\", \"Raw Data\", \"Pain\")\n",
    "AUGMENTED_DATA = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain\")\n",
    "AUGMENTED_DATA_TWOSTEP = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain Two-Step Augmentation\")\n",
    "AUGMENTED_DATA_FLEXIBLE = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Flexible Augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folder Structure\n",
    "First, we will duplicate the folder structure in \"Raw Data\" into \"Preprocessed Data\" and \"Augmented Data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate folder structure\n",
    "DL.mirror_folder_structure(RAW_DATA, PREPROCESSED_DATA)\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA)\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA_TWOSTEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original pain distribution\n",
    "img_paths = np.array(DL.get_image_paths(RAW_DATA))\n",
    "labels = np.array(DL.get_labels(img_paths))\n",
    "no_pain_labels = labels[labels[:,4].astype(int)==0]\n",
    "pain_labels = labels[labels[:,4].astype(int)>0]\n",
    "print(\"Pain Labels:\", len(pain_labels))\n",
    "print(\"No Pain Labels:\", len(no_pain_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of clients per group\n",
    "g1_img_paths = [x for x in os.listdir(os.path.join(RAW_DATA, \"group_1\")) if x != '.DS_Store']\n",
    "g2_img_paths = [x for x in os.listdir(os.path.join(RAW_DATA, \"group_2\")) if x != '.DS_Store']\n",
    "print(\"Group 1:\", len(g1_img_paths))\n",
    "print(\"Group 2:\", len(g2_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of sessions per client\n",
    "g1_img_paths = np.array(DL.get_image_paths(os.path.join(RAW_DATA, \"group_1\")))\n",
    "g2_img_paths = np.array(DL.get_image_paths(os.path.join(RAW_DATA, \"group_2\")))\n",
    "g1_labels = np.array(DL.get_labels(g1_img_paths))\n",
    "g2_labels = np.array(DL.get_labels(g2_img_paths))\n",
    "df_1 = pd.DataFrame(g1_labels, columns=['Person','Session','Culture','Frame','Pain']).astype(int)\n",
    "df_2 = pd.DataFrame(g2_labels, columns=['Person','Session','Culture','Frame','Pain']).astype(int)\n",
    "df_1['Group'] = 1\n",
    "df_2['Group'] = 2\n",
    "df = pd.concat([df_1, df_2])\n",
    "sess_num = pd.DataFrame(df.groupby(['Person', 'Group'])['Session'].nunique()).sort_values(['Group','Person'])\n",
    "sess_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of sessions per group\n",
    "print(\"Average Sessions Group 1: {0:.2f}\".format(df_1.groupby('Person')['Session'].nunique().mean()))\n",
    "print(\"Average Sessions Group 2: {0:.2f}\".format(df_2.groupby('Person')['Session'].nunique().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pain / No Pain per group\n",
    "print(\"Group 1 Pain/No Pain/Ratio: {} | {}\".format(df[(df['Group'] == 1) & (df['Pain'] == 0)].count()[0], df[(df['Group'] == 1) & (df['Pain'] > 0)].count()[0]))\n",
    "print(\"Group 2 Pain/No Pain/Ratio: {} | {}\".format(df[(df['Group'] == 2) & (df['Pain'] == 0)].count()[0], df[(df['Group'] == 2) & (df['Pain'] > 0)].count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Images\n",
    "We will now process the images. Preprocessing includes converting to greyscale, and histogram equalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "IP.bulk_process_images(RAW_DATA, PREPROCESSED_DATA, \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip images and copy originals into augmented data folder\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_TWOSTEP, \".jpg\", \"flip\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_TWOSTEP, \".jpg\", \"original\", \"pain\", label_threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate Originals and flipped images, and ensure that naming conventions stay consistent\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_flipped.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_original.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_rename_files(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_rotated\", \"_straight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop images to same maximum width and height (10-degree rotation in previous step cropped rotated images \n",
    "# down to (215, 215), so this is chosen as a max width/height)\n",
    "IP.bulk_crop_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, (215, 215))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample augmented data\n",
    "DL.downsample_data(os.path.join(AUGMENTED_DATA_TWOSTEP, \"group_1\"))\n",
    "DL.downsample_data(os.path.join(AUGMENTED_DATA_TWOSTEP, \"group_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Group 2 Data\n",
    "Split Group 2 Data into 40% Test Data and an additional 60% Test Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame to enable filtering by labels\n",
    "img_paths = DL.get_image_paths(os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2'))\n",
    "labels = DL.get_labels(img_paths)\n",
    "df = pd.DataFrame(labels, columns=['Person','Session','Culture','Frame','Pain', 'Trans_1', 'Trans_2'])\n",
    "df[['Person','Session','Culture','Frame','Pain']] = df[['Person','Session','Culture','Frame','Pain']].astype(int)\n",
    "df['img_path'] = img_paths\n",
    "df[['Trans_1', 'Trans_2', 'img_path']] = df[['Trans_1', 'Trans_2', 'img_path']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset into Train and Test\n",
    "origin_path = os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2')\n",
    "train_path = os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2_train')\n",
    "test_path = os.path.join(AUGMENTED_DATA_TWOSTEP, 'group_2_test')\n",
    "DL.move_train_test_data(df, origin_path, train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that split is 40 / 60\n",
    "img_train = DL.get_image_paths(train_path)\n",
    "img_test = DL.get_image_paths(test_path)\n",
    "print(\"Test: {0:.2f}\".format(len(img_test) / (len(img_test) + len(img_train))))\n",
    "print(\"Train: {0:.2f}\".format(len(img_train) / (len(img_test) + len(img_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset (Randomly)\n",
    "Splitting the dataset into training data and test data, by sampling without replacement from the train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images and select 20% at random\n",
    "img_paths = DL.get_image_paths(AUGMENTED_DATA_TWOSTEP)\n",
    "np.random.shuffle(img_paths)\n",
    "split_idx = int(len(img_paths)*0.2)\n",
    "img_paths_test = img_paths[:split_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that each client is represented with ~20% in the test data set\n",
    "img_per_client_test = np.unique(np.array(DL.get_labels(img_paths_test))[:,0], return_counts=True)[1]\n",
    "img_per_client_total = np.unique(np.array(DL.get_labels(img_paths))[:,0], return_counts=True)[1]\n",
    "img_per_client_test / img_per_client_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the test set is balanced\n",
    "pain = np.array(img_paths_test)[np.array(DL.get_labels(img_paths_test))[:,4].astype(int) >= 1]\n",
    "pain_test_labels = np.array(DL.get_labels(pain))\n",
    "all_test_labels = np.array(DL.get_labels(img_paths_test))\n",
    "print(\"Test Pain Split:\",len(pain_labels) / len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the split for each client in the test set\n",
    "DL.print_pain_split_per_client(all_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move test data set into test folder\n",
    "for src in img_paths_test:\n",
    "    file = os.path.basename(src)\n",
    "    dest = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(src))), 'test')\n",
    "    if not os.path.isdir(dest):\n",
    "        os.mkdir(dest)\n",
    "    try:\n",
    "        os.rename(src, os.path.join(dest, file))\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible Data Augmentation\n",
    "Code snippets that allow to move doubly augmented data around quickly. Main purpose is to ensure that the same images in original or augmented form are not being used for training and testing at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Double-augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirror folder structure\n",
    "print(\"Mirror Folder Structure\")\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA_FLEXIBLE)\n",
    "\n",
    "# Flip images and copy originals into augmented data folder\n",
    "print(\"Flip Images\")\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_FLEXIBLE, \".jpg\", \"flip\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_FLEXIBLE, \".jpg\", \"original\", \"pain\", label_threshold=-1)\n",
    "\n",
    "# Rotate Originals and flipped images, and ensure that naming conventions stay consistent\n",
    "print(\"Rotate Images\")\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_flipped.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_original.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_rename_files(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_rotated\", \"_straight\")\n",
    "\n",
    "# Crop images to same maximum width and height (10-degree rotation in previous step cropped rotated images \n",
    "# down to (215, 215), so this is chosen as a max width/height)\n",
    "print(\"Crop Images\")\n",
    "IP.bulk_crop_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, (215, 215), \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reset Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving all images into the \"raw\" subfolder\n",
    "DL.reset_to_raw(AUGMENTED_DATA_FLEXIBLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting all empty folders\n",
    "DL.delete_empty_folders(AUGMENTED_DATA_FLEXIBLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image paths and corresponding labels into a dataframe\n",
    "img_paths = np.array(DL.get_image_paths(AUGMENTED_DATA_FLEXIBLE))\n",
    "labels = np.array(DL.get_labels(img_paths))\n",
    "df = pd.DataFrame(labels, columns=['Person','Session','Culture','Frame','Pain', 'Trans_1', 'Trans_2'])\n",
    "df[['Person','Session','Culture','Frame','Pain']] = df[['Person','Session','Culture','Frame','Pain']].astype(int)\n",
    "df['img_path'] = img_paths\n",
    "df[['Trans_1', 'Trans_2', 'img_path']] = df[['Trans_1', 'Trans_2', 'img_path']].astype(str)\n",
    "df = df.sort_values(['Person', 'Session', 'Frame', 'Trans_1', 'Trans_2'], ascending=[True, True, True, False, False]).reset_index(drop=True)\n",
    "df['temp_id'] = df['Person'].astype(str) + df['Session'].astype(str) + df['Frame'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Remove Subject 101 from the data\n",
    "Subject 101 only has negative examples \"0\" and will therefore show \"0%\" on metrics like \"Recall\" or \"Precision\", skewing output graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pain Labels Subject 101 :  0\n"
     ]
    }
   ],
   "source": [
    "# Proving that subject 101 only has 0 labels\n",
    "subject = 101\n",
    "print(\"# Pain Labels Subject {} : \".format(subject), np.sum(df[df['Person'] == subject]['Pain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing subject 101 from the data\n",
    "df = df[df['Person'] != 101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Redistribute Data for Training\n",
    "Do one of the subsection steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into two groups\n",
    "group_1 = [42, 47, 49, 66, 95, 97, 103, 106, 108, 121, 123, 124]\n",
    "df_1 = df[df['Person'].isin(group_1)]\n",
    "df_2 = df[df['Person'].isin(group_1) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Redistribute Naively\n",
    "In this step, the we will just downsample the data and put it into two groups, without accounting for potential duplicates in test and train data (e.g. \"original\" in train, and \"flipped\" in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1), random_state=123)\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample second group\n",
    "df_2_pain_1 = df_2[df_2['Pain'] > 0]\n",
    "df_2_pain_0 = df_2[df_2['Pain'] == 0].sample(len(df_2_pain_1), random_state=123)\n",
    "df_2_downsampled = pd.concat((df_2_pain_0, df_2_pain_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Redistribute - No Mutation Duplicates\n",
    "In this step the data is split so that the same image in a mutated form is not in train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1), random_state=123)\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Pain Frames into Train and Test 60 / 40\n",
    "np.random.seed(123)\n",
    "ratio = 0.6\n",
    "\n",
    "temp_ids_pain = df_2[df_2['Pain'] > 0]['temp_id'].unique()\n",
    "temp_ids_pain_train = np.random.choice(temp_ids_pain, int(ratio * len(temp_ids_pain)), replace=False)\n",
    "temp_ids_pain_test = temp_ids_pain[np.isin(temp_ids_pain, temp_ids_pain_train) == False]\n",
    "df_2_pain_train = df_2[df_2['temp_id'].isin(temp_ids_pain_train)]\n",
    "df_2_pain_test = df_2[df_2['temp_id'].isin(temp_ids_pain_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Pain Frames into Train and Test 60 / 40, with the same number of Train / Test Samples as Pain\n",
    "temp_ids_no_pain = df_2[df_2['Pain'] == 0]['temp_id'].unique()\n",
    "temp_ids_no_pain_train = np.random.choice(temp_ids_no_pain, len(df_2_pain_train), replace=False)\n",
    "temp_ids_no_pain_test = np.random.choice(temp_ids_no_pain[np.isin(temp_ids_no_pain, temp_ids_no_pain_train) == False], len(df_2_pain_test), replace=False)\n",
    "df_2_pain_0_train = df_2[df_2['temp_id'].isin(temp_ids_no_pain_train)].sample(len(df_2_pain_train))\n",
    "df_2_pain_0_test = df_2[df_2['temp_id'].isin(temp_ids_no_pain_test)].sample(len(df_2_pain_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test\n",
    "df_2_train = pd.concat((df_2_pain_train, df_2_pain_0_train))\n",
    "df_2_test = pd.concat((df_2_pain_test, df_2_pain_0_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:          60% | Test:          40%\n",
      "Train No Pain: 7396 | Test No Pain: 4936\n",
      "Train Pain:    7396 | Test Pain:    4936\n",
      "Train Total:  14792 | Test Total:   9872\n",
      "\n",
      "Total:        24664\n",
      "----------------------------------------\n",
      "Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify that everything went well\n",
    "print(\"Train:          {:.0%} |\".format(len(df_2_train) / (len(df_2_test) + len(df_2_train))),\n",
    "      \"Test:          {:.0%}\".format(len(df_2_test) / (len(df_2_test) + len(df_2_train))), )\n",
    "print(\"Train No Pain: {} |\".format(len(df_2_train[df_2_train['Pain'] == 0])), \"Test No Pain: {}\".format(len(df_2_test[df_2_test['Pain'] == 0])))\n",
    "print(\"Train Pain:    {} |\".format(len(df_2_train[df_2_train['Pain'] > 0])),  \"Test Pain:    {}\".format(len(df_2_test[df_2_test['Pain'] > 0])))\n",
    "print(\"Train Total:  {} |\".format(len(df_2_train)), \"Test Total:   {}\".format(len(df_2_test)))\n",
    "print()\n",
    "print(\"Total:        {}\".format(len(df_2_train) + len(df_2_test)))\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Duplicates:\", sum(df_2_train['temp_id'].isin(df_2_test['temp_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Allocate Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_group(df, path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    for f_path in df['img_path'].values:\n",
    "        os.rename(f_path, os.path.join(path, os.path.basename(f_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate Group 1\n",
    "group_1_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_1\")\n",
    "allocate_group(df_1_downsampled, group_1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate Group 2 Train / Test\n",
    "train_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_train')\n",
    "test_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_test')\n",
    "\n",
    "allocate_group(df_2_train, train_path)\n",
    "allocate_group(df_2_test, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1:        41416\n",
      "Group 2 Train:  14792\n",
      "Group 2 Test:   9872\n",
      "Raw:            126344\n",
      "Raw Pain Img's: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify Success\n",
    "print(\"Group 1:        {}\".format(len(os.listdir(group_1_path))))\n",
    "print(\"Group 2 Train:  {}\".format(len(os.listdir(train_path))))\n",
    "print(\"Group 2 Test:   {}\".format(len(os.listdir(test_path))))\n",
    "print(\"Raw:            {}\".format(len(os.listdir(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'raw')))))\n",
    "print(\"Raw Pain Img's: {}\".format(np.sum(np.minimum(np.array(DL.get_labels(DL.get_image_paths(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'raw'))))[:,4].astype(int), 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_experiment(experiment):\n",
    "    print(\"\\n\\n\\033[1m{} Experiment {} {}\\033[0m\".format(\"-\"*int((130-len(experiment))/2), \n",
    "                                                         experiment, \"-\"*int((130-len(experiment))/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m----------------------------------------------- Experiment 6 - Centralized without pre-training -----------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "exp = \"6 - Centralized without pre-training\"\n",
    "print_experiment(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FederatedLearning",
   "language": "python",
   "name": "federatedlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
