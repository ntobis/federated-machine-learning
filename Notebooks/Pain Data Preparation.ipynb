{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pain Data Preparation\n",
    "This notebook prepares the pain dataset in to be able to successfully train a convolutional neural network. Data augmentation techniques such as greyscaling, histogram equalization, etc. are employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from Scripts import Data_Loader_Functions as DL\n",
    "from Scripts import Image_Processor as IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder paths\n",
    "RAW_DATA = os.path.join(module_path, \"Data\", \"Raw Data\", \"Pain\")\n",
    "AUGMENTED_DATA = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain\")\n",
    "AUGMENTED_DATA_TWOSTEP = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Pain Two-Step Augmentation\")\n",
    "AUGMENTED_DATA_FLEXIBLE = os.path.join(module_path, \"Data\", \"Augmented Data\", \"Flexible Augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original pain distribution\n",
    "img_paths = np.array(DL.get_image_paths(RAW_DATA))\n",
    "labels = np.array(DL.get_labels(img_paths))\n",
    "no_pain_labels = labels[labels[:,4].astype(int)==0]\n",
    "pain_labels = labels[labels[:,4].astype(int)>0]\n",
    "print(\"Pain Labels:\", len(pain_labels))\n",
    "print(\"No Pain Labels:\", len(no_pain_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of clients per group\n",
    "g1_img_paths = [x for x in os.listdir(os.path.join(RAW_DATA, \"group_1\")) if x != '.DS_Store']\n",
    "g2_img_paths = [x for x in os.listdir(os.path.join(RAW_DATA, \"group_2\")) if x != '.DS_Store']\n",
    "print(\"Group 1:\", len(g1_img_paths))\n",
    "print(\"Group 2:\", len(g2_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of sessions per client\n",
    "g1_img_paths = np.array(DL.get_image_paths(os.path.join(RAW_DATA, \"group_1\")))\n",
    "g2_img_paths = np.array(DL.get_image_paths(os.path.join(RAW_DATA, \"group_2\")))\n",
    "g1_labels = np.array(DL.get_labels(g1_img_paths))\n",
    "g2_labels = np.array(DL.get_labels(g2_img_paths))\n",
    "df_1 = pd.DataFrame(g1_labels, columns=['Person','Session','Culture','Frame','Pain']).astype(int)\n",
    "df_2 = pd.DataFrame(g2_labels, columns=['Person','Session','Culture','Frame','Pain']).astype(int)\n",
    "df_1['Group'] = 1\n",
    "df_2['Group'] = 2\n",
    "df = pd.concat([df_1, df_2])\n",
    "sess_num = pd.DataFrame(df.groupby(['Person', 'Group'])['Session'].nunique()).sort_values(['Group','Person'])\n",
    "sess_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of sessions per group\n",
    "print(\"Average Sessions Group 1: {0:.2f}\".format(df_1.groupby('Person')['Session'].nunique().mean()))\n",
    "print(\"Average Sessions Group 2: {0:.2f}\".format(df_2.groupby('Person')['Session'].nunique().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pain / No Pain per group\n",
    "print(\"Group 1 Pain/No Pain/Ratio: {} | {}\".format(df[(df['Group'] == 1) & (df['Pain'] == 0)].count()[0], df[(df['Group'] == 1) & (df['Pain'] > 0)].count()[0]))\n",
    "print(\"Group 2 Pain/No Pain/Ratio: {} | {}\".format(df[(df['Group'] == 2) & (df['Pain'] == 0)].count()[0], df[(df['Group'] == 2) & (df['Pain'] > 0)].count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Images\n",
    "We will now process the images. Preprocessing includes converting to greyscale, and histogram equalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "IP.bulk_process_images(RAW_DATA, PREPROCESSED_DATA, \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip images and copy originals into augmented data folder\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_TWOSTEP, \".jpg\", \"flip\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_TWOSTEP, \".jpg\", \"original\", \"pain\", label_threshold=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate Originals and flipped images, and ensure that naming conventions stay consistent\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_flipped.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_original.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_rename_files(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, \"_rotated\", \"_straight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop images to same maximum width and height (10-degree rotation in previous step cropped rotated images \n",
    "# down to (215, 215), so this is chosen as a max width/height)\n",
    "IP.bulk_crop_images(AUGMENTED_DATA_TWOSTEP, AUGMENTED_DATA_TWOSTEP, (215, 215))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample augmented data\n",
    "DL.downsample_data(os.path.join(AUGMENTED_DATA_TWOSTEP, \"group_1\"))\n",
    "DL.downsample_data(os.path.join(AUGMENTED_DATA_TWOSTEP, \"group_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible Data Augmentation\n",
    "Code snippets that allow to move doubly augmented data around quickly. Main purpose is to ensure that the same images in original or augmented form are not being used for training and testing at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Double-augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirror folder structure\n",
    "print(\"Mirror Folder Structure\")\n",
    "DL.mirror_folder_structure(RAW_DATA, AUGMENTED_DATA_FLEXIBLE)\n",
    "\n",
    "# Flip images and copy originals into augmented data folder\n",
    "print(\"Flip Images\")\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_FLEXIBLE, \".jpg\", \"flip\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(PREPROCESSED_DATA, AUGMENTED_DATA_FLEXIBLE, \".jpg\", \"original\", \"pain\", label_threshold=-1)\n",
    "\n",
    "# Rotate Originals and flipped images, and ensure that naming conventions stay consistent\n",
    "print(\"Rotate Images\")\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_flipped.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_augment_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_original.jpg\", \"rotate_crop\", \"pain\", label_threshold=-1)\n",
    "IP.bulk_rename_files(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, \"_rotated\", \"_straight\")\n",
    "\n",
    "# Crop images to same maximum width and height (10-degree rotation in previous step cropped rotated images \n",
    "# down to (215, 215), so this is chosen as a max width/height)\n",
    "print(\"Crop Images\")\n",
    "IP.bulk_crop_images(AUGMENTED_DATA_FLEXIBLE, AUGMENTED_DATA_FLEXIBLE, (215, 215), \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reset Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving all images into the \"raw\" subfolder\n",
    "DL.reset_to_raw(AUGMENTED_DATA_FLEXIBLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting all empty folders\n",
    "DL.delete_empty_folders(AUGMENTED_DATA_FLEXIBLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image paths and corresponding labels into a dataframe\n",
    "img_paths = np.array(DL.get_image_paths(AUGMENTED_DATA_FLEXIBLE))\n",
    "labels = np.array(DL.get_labels(img_paths))\n",
    "df = pd.DataFrame(labels, columns=['Person','Session','Culture','Frame','Pain', 'Trans_1', 'Trans_2'])\n",
    "df[['Person','Session','Culture','Frame','Pain']] = df[['Person','Session','Culture','Frame','Pain']].astype(int)\n",
    "df['img_path'] = img_paths\n",
    "df[['Trans_1', 'Trans_2', 'img_path']] = df[['Trans_1', 'Trans_2', 'img_path']].astype(str)\n",
    "df = df.sort_values(['Person', 'Session', 'Frame', 'Trans_1', 'Trans_2'], ascending=[True, True, True, False, False]).reset_index(drop=True)\n",
    "df['temp_id'] = df['Person'].astype(str) + df['Session'].astype(str) + df['Frame'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Remove Subject 101 from the data\n",
    "Subject 101 only has negative examples \"0\" and will therefore show \"0%\" on metrics like \"Recall\" or \"Precision\", skewing output graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pain Labels Subject 101 :  0\n"
     ]
    }
   ],
   "source": [
    "# Proving that subject 101 only has 0 labels\n",
    "subject = 101\n",
    "print(\"# Pain Labels Subject {} : \".format(subject), np.sum(df[df['Person'] == subject]['Pain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing subject 101 from the data\n",
    "df = df[df['Person'] != 101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Redistribute Data for Training\n",
    "Do one of the subsection steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution checking\n",
    "def print_distribution(df_train, df_test):\n",
    "    print(\"\\033[1mTrain\\t\\t\\t\\t   |Test\\033[0m\")\n",
    "    for train, test in zip(df_train.groupby('Person'), df_test.groupby('Person')):\n",
    "        print(\"Subject {} Train:\\t{}\\t{:.0%}|{:.0%}  Subject {} Test:\\t{}\"\n",
    "              .format(train[0], len(train[1]), len(train[1]) / (len(train[1]) + len(test[1])),\n",
    "                      len(test[1]) / (len(train[1]) + len(test[1])), test[0], len(test[1])))\n",
    "    print(\"-\" * 68)\n",
    "    print(\"Total Original Train:\\t{}\\t{:.0%}|{:.0%}  Total Original Test:\\t{}\"\n",
    "          .format(len(df_train), len(df_train) / (len(df_train) + len(df_test)),\n",
    "                  len(df_test) / (len(df_train) + len(df_test)), len(df_test)))\n",
    "\n",
    "def print_pain_distribution(df_train, df_test):\n",
    "    print(\"Train:          {:.0%} |\".format(len(df_train) / (len(df_test) + len(df_train))),\n",
    "          \"Test:          {:.0%}\".format(len(df_test) / (len(df_test) + len(df_train))), )\n",
    "    print(\"Train No Pain: {} |\".format(len(df_train[df_train['Pain'] == 0])),\n",
    "          \"Test No Pain: {}\".format(len(df_test[df_test['Pain'] == 0])))\n",
    "    print(\"Train Pain:    {} |\".format(len(df_train[df_train['Pain'] > 0])),\n",
    "          \"Test Pain:    {}\".format(len(df_test[df_test['Pain'] > 0])))\n",
    "    print(\"Train Total:  {} |\".format(len(df_train)), \"Test Total:   {}\".format(len(df_test)))\n",
    "    print()\n",
    "    print(\"Total:        {}\".format(len(df_train) + len(df_test)))\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Duplicates:\", sum(df_train['temp_id'].isin(df_test['temp_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into two groups\n",
    "group_1 = [42, 47, 49, 66, 95, 97, 103, 106, 108, 121, 123, 124]\n",
    "df_1 = df[df['Person'].isin(group_1)]\n",
    "df_2 = df[~df['Person'].isin(group_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43,  48,  52,  59,  64,  80,  92,  96, 107, 109, 115, 120])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2['Person'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Redistribute Naively\n",
    "In this step, the we will just downsample the data and put it into two groups, without accounting for potential duplicates in test and train data (e.g. \"original\" in train, and \"flipped\" in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1))\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample second group\n",
    "df_2_pain_1 = df_2[df_2['Pain'] > 0]\n",
    "df_2_pain_0 = df_2[df_2['Pain'] == 0].sample(len(df_2_pain_1))\n",
    "df_2_downsampled = pd.concat((df_2_pain_0, df_2_pain_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Redistribute - No Mutation Duplicates\n",
    "In this step the data is split so that the same image in a mutated form is not in train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1))\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Pain Frames into Train and Test 60 / 40\n",
    "ratio = 0.6\n",
    "\n",
    "temp_ids_pain = df_2[df_2['Pain'] > 0]['temp_id'].unique()\n",
    "temp_ids_pain_train = np.random.choice(temp_ids_pain, int(ratio * len(temp_ids_pain)), replace=False)\n",
    "temp_ids_pain_test = temp_ids_pain[np.isin(temp_ids_pain, temp_ids_pain_train) == False]\n",
    "df_2_pain_train = df_2[df_2['temp_id'].isin(temp_ids_pain_train)]\n",
    "df_2_pain_test = df_2[df_2['temp_id'].isin(temp_ids_pain_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Pain Frames into Train and Test 60 / 40, with the same number of Train / Test Samples as Pain\n",
    "temp_ids_no_pain = df_2[df_2['Pain'] == 0]['temp_id'].unique()\n",
    "temp_ids_no_pain_train = np.random.choice(temp_ids_no_pain, len(df_2_pain_train), replace=False)\n",
    "temp_ids_no_pain_test = np.random.choice(temp_ids_no_pain[np.isin(temp_ids_no_pain, temp_ids_no_pain_train) == False], len(df_2_pain_test), replace=False)\n",
    "df_2_pain_0_train = df_2[df_2['temp_id'].isin(temp_ids_no_pain_train)].sample(len(df_2_pain_train))\n",
    "df_2_pain_0_test = df_2[df_2['temp_id'].isin(temp_ids_no_pain_test)].sample(len(df_2_pain_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test\n",
    "df_2_train = pd.concat((df_2_pain_train, df_2_pain_0_train))\n",
    "df_2_test = pd.concat((df_2_pain_test, df_2_pain_0_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that everything went well\n",
    "print_pain_distribution(df_2_train, df_2_test)\n",
    "print()\n",
    "print_distribution(df_2_train, df_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3: Redistribute - No Test Augmentation\n",
    "In this step we redistribute the data so that only the train dataset is balanced. The test dataset maintains its original composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1))\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_originals = df_2[(df_2['Trans_1'] == 'original') & (df_2['Trans_2'] == 'straight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split original images into train and test, on a per person basis, 60/40\n",
    "ratio = 0.6\n",
    "\n",
    "df_2_originals_train = pd.DataFrame(columns=df_2_originals.columns)\n",
    "df_2_originals_test = pd.DataFrame(columns=df_2_originals.columns)\n",
    "for df_person in df_2_originals.groupby('Person'):\n",
    "    df_person_train = df_person[1].sample(frac=ratio)\n",
    "    df_person_test = df_person[1].drop(df_person_train.index)\n",
    "    df_2_originals_train = pd.concat((df_2_originals_train, df_person_train))\n",
    "    df_2_originals_test = pd.concat((df_2_originals_test, df_person_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:          60% | Test:          40%\n",
      "Train No Pain: 9861 | Test No Pain: 6601\n",
      "Train Pain:    1866 | Test Pain:    1217\n",
      "Train Total:  11727 | Test Total:   7818\n",
      "\n",
      "Total:        19545\n",
      "----------------------------------------\n",
      "Duplicates: 0\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "\u001b[1mTrain\t\t\t\t   |Test\u001b[0m\n",
      "Subject 43 Train:\t672\t60%|40%  Subject 43 Test:\t448\n",
      "Subject 48 Train:\t529\t60%|40%  Subject 48 Test:\t353\n",
      "Subject 52 Train:\t1565\t60%|40%  Subject 52 Test:\t1044\n",
      "Subject 59 Train:\t464\t60%|40%  Subject 59 Test:\t309\n",
      "Subject 64 Train:\t929\t60%|40%  Subject 64 Test:\t620\n",
      "Subject 80 Train:\t1178\t60%|40%  Subject 80 Test:\t786\n",
      "Subject 92 Train:\t901\t60%|40%  Subject 92 Test:\t601\n",
      "Subject 96 Train:\t1412\t60%|40%  Subject 96 Test:\t941\n",
      "Subject 107 Train:\t1225\t60%|40%  Subject 107 Test:\t816\n",
      "Subject 109 Train:\t1142\t60%|40%  Subject 109 Test:\t761\n",
      "Subject 115 Train:\t770\t60%|40%  Subject 115 Test:\t513\n",
      "Subject 120 Train:\t940\t60%|40%  Subject 120 Test:\t626\n",
      "--------------------------------------------------------------------\n",
      "Total Original Train:\t11727\t60%|40%  Total Original Test:\t7818\n"
     ]
    }
   ],
   "source": [
    "# Verify that everything went well in this first stage. Expected output is a 60/40 split for every subject,\n",
    "# as well as significantly more 'no pain' than 'pain' subjects\n",
    "print_pain_distribution(df_2_originals_train, df_2_originals_test)\n",
    "print(\"\\n--------------------------------------------------------------------\\n\")\n",
    "print_distribution(df_2_originals_train, df_2_originals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_train_ids = df_2_originals_train['temp_id'].unique()\n",
    "df_2_train = df_2[df_2['temp_id'].isin(df_2_train_ids)]\n",
    "df_2_train_pain = df_2_train[df_2_train['Pain'] > 0]\n",
    "df_2_train_no_pain = df_2_train[df_2_train['Pain'] == 0].sample(len(df_2_train_pain))\n",
    "df_2_train = pd.concat((df_2_train_pain, df_2_train_no_pain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_test = df_2_originals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:          66% | Test:          34%\n",
      "Train No Pain: 7464 | Test No Pain: 6601\n",
      "Train Pain:    7464 | Test Pain:    1217\n",
      "Train Total:  14928 | Test Total:   7818\n",
      "\n",
      "Total:        22746\n",
      "----------------------------------------\n",
      "Duplicates: 0\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "\u001b[1mTrain\t\t\t\t   |Test\u001b[0m\n",
      "Subject 43 Train:\t686\t60%|40%  Subject 43 Test:\t448\n",
      "Subject 48 Train:\t569\t62%|38%  Subject 48 Test:\t353\n",
      "Subject 52 Train:\t1421\t58%|42%  Subject 52 Test:\t1044\n",
      "Subject 59 Train:\t585\t65%|35%  Subject 59 Test:\t309\n",
      "Subject 64 Train:\t1054\t63%|37%  Subject 64 Test:\t620\n",
      "Subject 80 Train:\t3005\t79%|21%  Subject 80 Test:\t786\n",
      "Subject 92 Train:\t1583\t72%|28%  Subject 92 Test:\t601\n",
      "Subject 96 Train:\t1387\t60%|40%  Subject 96 Test:\t941\n",
      "Subject 107 Train:\t1779\t69%|31%  Subject 107 Test:\t816\n",
      "Subject 109 Train:\t1205\t61%|39%  Subject 109 Test:\t761\n",
      "Subject 115 Train:\t784\t60%|40%  Subject 115 Test:\t513\n",
      "Subject 120 Train:\t870\t58%|42%  Subject 120 Test:\t626\n",
      "--------------------------------------------------------------------\n",
      "Total Original Train:\t14928\t66%|34%  Total Original Test:\t7818\n"
     ]
    }
   ],
   "source": [
    "# Print final distribution with augmented train images\n",
    "print_pain_distribution(df_2_train, df_2_test)\n",
    "print(\"\\n--------------------------------------------------------------------\\n\")\n",
    "print_distribution(df_2_train, df_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pain = df_2_train[df_2_train['Pain']>0].groupby('Person').count().T\n",
    "df_no_pain = df_2_train[df_2_train['Pain']==0].groupby('Person').count().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat((df_pain[:1], df_no_pain[:1]))\n",
    "df_test['Total'] = df_test.sum(axis=1)\n",
    "df_test = pd.concat((df_test, df_test[:1] / df_test.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Person</th>\n",
       "      <th>43</th>\n",
       "      <th>48</th>\n",
       "      <th>52</th>\n",
       "      <th>59</th>\n",
       "      <th>64</th>\n",
       "      <th>80</th>\n",
       "      <th>92</th>\n",
       "      <th>96</th>\n",
       "      <th>107</th>\n",
       "      <th>109</th>\n",
       "      <th>115</th>\n",
       "      <th>120</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Session</th>\n",
       "      <td>212.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>2596.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>432.000000</td>\n",
       "      <td>236.00000</td>\n",
       "      <td>192.00000</td>\n",
       "      <td>7464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session</th>\n",
       "      <td>474.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>1141.000000</td>\n",
       "      <td>273.000000</td>\n",
       "      <td>654.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>995.000000</td>\n",
       "      <td>695.000000</td>\n",
       "      <td>773.000000</td>\n",
       "      <td>548.00000</td>\n",
       "      <td>678.00000</td>\n",
       "      <td>7464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session</th>\n",
       "      <td>0.309038</td>\n",
       "      <td>0.351494</td>\n",
       "      <td>0.197044</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.379507</td>\n",
       "      <td>0.863894</td>\n",
       "      <td>0.712571</td>\n",
       "      <td>0.282624</td>\n",
       "      <td>0.609331</td>\n",
       "      <td>0.358506</td>\n",
       "      <td>0.30102</td>\n",
       "      <td>0.22069</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Person           43          48           52          59          64  \\\n",
       "Session  212.000000  200.000000   280.000000  312.000000  400.000000   \n",
       "Session  474.000000  369.000000  1141.000000  273.000000  654.000000   \n",
       "Session    0.309038    0.351494     0.197044    0.533333    0.379507   \n",
       "\n",
       "Person            80           92          96          107         109  \\\n",
       "Session  2596.000000  1128.000000  392.000000  1084.000000  432.000000   \n",
       "Session   409.000000   455.000000  995.000000   695.000000  773.000000   \n",
       "Session     0.863894     0.712571    0.282624     0.609331    0.358506   \n",
       "\n",
       "Person         115        120   Total  \n",
       "Session  236.00000  192.00000  7464.0  \n",
       "Session  548.00000  678.00000  7464.0  \n",
       "Session    0.30102    0.22069     0.5  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.4: Original Distribution\n",
    "In this step, the data used is only the original images that have been preprocessed but not augmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original images\n",
    "df_1_original = df_1[(df_1['Trans_1'] == 'original') & (df_1['Trans_2'] == 'straight')]\n",
    "df_2_original = df_2[(df_2['Trans_1'] == 'original') & (df_2['Trans_2'] == 'straight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_2 into train and test on a per person basis\n",
    "ratio = 0.6\n",
    "\n",
    "df_2_originals_train = pd.DataFrame(columns=df_2_original.columns)\n",
    "df_2_originals_test = pd.DataFrame(columns=df_2_original.columns)\n",
    "for df_person in df_2_original.groupby('Person'):\n",
    "    df_person_train = df_person[1].sample(frac=ratio)\n",
    "    df_person_test = df_person[1].drop(df_person_train.index)\n",
    "    df_2_originals_train = pd.concat((df_2_originals_train, df_person_train))\n",
    "    df_2_originals_test = pd.concat((df_2_originals_test, df_person_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_train = df_2_originals_train\n",
    "df_2_test = df_2_originals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Group 1 Distribution\n",
    "print(\"Group 1 Pain:    \", np.sum(df_1_original['Pain'] > 0),\"\\t|  {:.0%}\".format(np.sum(df_1_original['Pain'] > 0) / len(df_1_original)))\n",
    "print(\"Group 1 No Pain: \", np.sum(df_1_original['Pain'] == 0),\"|  {:.0%}\".format(np.sum(df_1_original['Pain'] == 0) / len(df_1_original)))\n",
    "print(\"\\n\\t\\tPain \\t| No Pain\\tPain\\t| No Pain\")\n",
    "print(\"-\"*35,\"-\"*25)\n",
    "for df_person in df_1_original.groupby('Person'):\n",
    "    print(\"Person {}:\\t{}\\t| {}\\t\\t{:.0%}\\t|  {:.0%}\".format(df_person[0], np.sum(df_person[1]['Pain'] > 0), np.sum(df_person[1]['Pain'] == 0), np.sum(df_person[1]['Pain'] > 0) / len(df_person[1]), np.sum(df_person[1]['Pain'] == 0) / len(df_person[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final distribution with augmented train images\n",
    "print_pain_distribution(df_2_originals_train, df_2_originals_test)\n",
    "print(\"\\n--------------------------------------------------------------------\\n\")\n",
    "print_distribution(df_2_originals_train, df_2_originals_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.4: Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample first group\n",
    "df_1_pain_1 = df_1[df_1['Pain'] > 0]\n",
    "df_1_pain_0 = df_1[df_1['Pain'] == 0].sample(len(df_1_pain_1))\n",
    "df_1_downsampled = pd.concat((df_1_pain_0, df_1_pain_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into sessions\n",
    "session_dfs_2 = np.array([idx_df for idx_df in df_2.groupby('Session')])\n",
    "session_paths = [os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_2\", \"session_\" + str(sess)) for sess in session_dfs_2[:,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Allocate Groups\n",
    "Move the image files into the correct folders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_group(df, path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    for f_path in df['img_path'].values:\n",
    "        os.rename(f_path, os.path.join(path, os.path.basename(f_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate Group 1\n",
    "group_1_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_1\")\n",
    "allocate_group(df_1_downsampled, group_1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1: Allocate group 2 into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate Group 2 Train / Test\n",
    "train_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_train')\n",
    "test_path = os.path.join(AUGMENTED_DATA_FLEXIBLE, 'group_2_test')\n",
    "\n",
    "allocate_group(df_2_train, train_path)\n",
    "allocate_group(df_2_test, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Success, expected outcome is no instances of pain images in the \"Raw\" folder, a large group one,\n",
    "# and smaller group 2 train and test\n",
    "print(\"Group 1:        {}\".format(len(os.listdir(group_1_path))))\n",
    "print(\"Group 2 Train:  {}\".format(len(os.listdir(train_path))))\n",
    "print(\"Group 2 Test:   {}\".format(len(os.listdir(test_path))))\n",
    "print(\"Raw:            {}\".format(len(os.listdir(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'raw')))))\n",
    "print(\"Raw Pain Img's: {}\".format(np.sum(np.minimum(np.array(DL.get_labels(DL.get_image_paths(os.path.join(AUGMENTED_DATA_FLEXIBLE, 'raw'))))[:,4].astype(int), 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2: Allocation group 2 into sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate into sessions\n",
    "if not os.path.isdir(os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_2\")):\n",
    "    os.mkdir(os.path.join(AUGMENTED_DATA_FLEXIBLE, \"group_2\"))\n",
    "for df, path in zip(session_dfs_2[:,1], session_paths):\n",
    "    allocate_group(df, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Success\n",
    "for path in sorted(session_paths):\n",
    "    print(\"Session {}:\".format(path[-1]), len(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0.6, 0.4], [1, 0], [1, 0], [0, 1]])\n",
    "b = np.array([[0.6, 0.4], [0.4, 0.6], [0.3, 0.7], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = tf.keras.metrics.TruePositives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/confusion_matrix.py:193: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/confusion_matrix.py:194: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'confusion_matrix/SparseTensorDenseAdd:0' shape=(5, 5) dtype=int32>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "    print(tf.math.confusion_matrix(np.array([0, 1]), np.array([0, 0])).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TP(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, 1)\n",
    "    y_true = tf.argmax(y_true, 1)\n",
    "    return tf.math.count_nonzero(y_pred * y_true)\n",
    "\n",
    "\n",
    "def FP(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, 1)\n",
    "    y_true = tf.argmax(y_true, 1)\n",
    "    return tf.math.count_nonzero(y_pred * (y_true - 1))\n",
    "\n",
    "\n",
    "def TN(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, 1)\n",
    "    y_true = tf.argmax(y_true, 1)\n",
    "    return tf.math.count_nonzero((y_pred - 1) * (y_true - 1))\n",
    "\n",
    "\n",
    "def FN(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, 1)\n",
    "    y_true = tf.argmax(y_true, 1)\n",
    "    return tf.math.count_nonzero((y_pred - 1) * y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
