{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tf.keras.models  # like 'from tensorflow.keras import models' (PyCharm import issue workaround)\n",
    "layers = tf.keras.layers  # like 'from tensorflow.keras import layers' (PyCharm import issue workaround)\n",
    "optimizers = tf.keras.optimizers  # like 'from tensorflow.keras import optimizers' (PyCharm import issue workaround)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    \"\"\"\n",
    "    Loads the MNIST Data Set and reshapes it for further model training\n",
    "\n",
    "    :return:\n",
    "        train_images        numpy array of shape (60000, 28, 28, 1)\n",
    "        train_labels        numpy array of shape (60000, )\n",
    "        test_images         numpy array of shape (10000, 28, 28, 1)\n",
    "        test_labels         numpy array of shape (10000, )\n",
    "    \"\"\"\n",
    "\n",
    "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "    test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape):\n",
    "    \"\"\"\n",
    "    Compile and return a simple CNN model for image recognition.\n",
    "\n",
    "    Configuration:\n",
    "    Layer 1: Convolution Layer | Filters: 32 | Kernel Size: 3x3 | Activation: Relu\n",
    "    Layer 2: Max Pooling Layer | Filter: 2x2\n",
    "    Layer 3: Dense Layer       | Neurons: 32 | Activation: Relu\n",
    "    Layer 4: Dense Layer       | Neurons: 10 | Activation: Softmax\n",
    "\n",
    "    Optimizer:      Adam\n",
    "    Loss function:  Sparse Categorical Cross Entropy\n",
    "    Loss metric:    Accuracy\n",
    "\n",
    "\n",
    "    :param input_shape:     image input shape (tuple), e.g. (28, 28, 1)\n",
    "\n",
    "    :return:\n",
    "        model               compiled tensorflow model\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up model type\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add layers, inspired by https://www.tensorflow.org/beta/tutorials/images/intro_to_cnns\n",
    "    model.add(layers.Conv2D(32, (5, 5), input_shape=input_shape, kernel_initializer='random_normal'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (5, 5), input_shape=input_shape, kernel_initializer='random_normal'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu', kernel_initializer='random_normal'))\n",
    "    model.add(layers.Dense(10, activation='softmax', kernel_initializer='random_normal'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vname_to_FeedPname(var):\n",
    "    return var.name[:var.name.find(':')] + '_placeholder:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsAccountant:\n",
    "    def __init__(self, weights, clients, model):\n",
    "        self.Global_Weights = weights\n",
    "        self.Local_Weights = []\n",
    "        self.Local_Updates = []\n",
    "        self.Local_Norms = []\n",
    "        self.Clipped_Updates = None\n",
    "        self.Global_Updates = None\n",
    "        self.median = None\n",
    "        self.num_clients = clients\n",
    "        \n",
    "    def append_local_weights(self, weights):\n",
    "        self.Local_Weights.append(weights)\n",
    "\n",
    "    def average_local_weights(self):\n",
    "        self.Local_Weights = np.array(self.Local_Weights).T\n",
    "        self.Global_Weights = np.mean(self.Local_Weights, axis=1)\n",
    "        del self.Local_Weights\n",
    "        self.Local_Weights = []\n",
    "        return self.Global_Weights\n",
    "\n",
    "    def get_global_weights(self):\n",
    "        return self.Global_Weights\n",
    "\n",
    "    def compute_local_updates(self):\n",
    "        for local_model in self.Local_Weights:\n",
    "            updates = [glob - loc for glob, loc in zip(self.Global_Weights, local_model)]\n",
    "            self.Local_Updates.append(updates)\n",
    "        \n",
    "    def compute_update_norm(self):\n",
    "        self.Local_Norms = []\n",
    "        for update in self.Local_Updates:\n",
    "            l2_norm = [np.sqrt(np.sum(np.square(layer))) for layer in update]\n",
    "            self.Local_Norms.append(l2_norm)\n",
    "        self.Local_Norms = np.array(self.Local_Norms)\n",
    "        \n",
    "    def clip_updates(self):\n",
    "        self.median = np.median(self.Local_Norms, axis=0)\n",
    "        scaling_factor = np.divide(self.Local_Norms, self.median, where=self.median > 0)\n",
    "        scaling_factor = np.maximum(scaling_factor, 1)\n",
    "        self.Clipped_Updates = np.divide(self.Local_Updates, scaling_factor)\n",
    "\n",
    "    def reset_local_parameters(self):\n",
    "        del self.Local_Weights\n",
    "        del self.Local_Updates\n",
    "        del self.Local_Norms\n",
    "        del self.Clipped_Updates\n",
    "        self.Local_Weights = []\n",
    "        self.Local_Updates = []\n",
    "        self.Local_Norms = []\n",
    "        self.Clipped_Updates = []\n",
    "\n",
    "    def average_local_clipped_updates(self):\n",
    "        # self.Local_Updates = np.array(self.Clipped_Updates).T\n",
    "        self.Global_Updates = np.mean(np.array(self.Local_Updates).T, axis=1)\n",
    "        self.reset_local_parameters()\n",
    "\n",
    "    def compute_noisy_global_weights(self, sigma):\n",
    "        self.Global_Weights = []\n",
    "        for layer, median, weights in zip(self.Global_Updates, self.median, self.Global_Weights):\n",
    "            noise = np.random.normal(loc=0, scale=sigma * median, size=layer.shape) / self.num_clients\n",
    "            noise = 0\n",
    "            noisy_update = layer + noise\n",
    "            self.Global_Weights.append(noisy_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsAccountant_2:\n",
    "    def __init__(self,sess,model,Sigma, real_round):\n",
    "\n",
    "        self.Weights = [np.expand_dims(sess.run(v), -1) for v in tf.trainable_variables()]\n",
    "        self.keys = [Vname_to_FeedPname(v) for v in tf.trainable_variables()]\n",
    "\n",
    "        # The trainable parameters are [q x p] matrices, we expand them to [q x p x 1] in order to later stack them\n",
    "        # along the third dimension.\n",
    "\n",
    "        # Create a list out of the model dictionary in the order in which the graph holds them:\n",
    "\n",
    "        self.global_model = [model[k] for k in self.keys]\n",
    "        self.Sigma = Sigma\n",
    "        self.Updates = []\n",
    "        self.median = []\n",
    "        self.Norms = []\n",
    "        self.ClippedUpdates = []\n",
    "        self.m = 0.0\n",
    "        self.num_weights = len(self.Weights)\n",
    "        self.round = real_round\n",
    "\n",
    "    def save_params(self,save_dir):\n",
    "        filehandler = open(save_dir + '/Wweights_accountant_round_'+self.round + '.pkl', \"wb\")\n",
    "        pickle.dump(self, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "    def allocate(self, sess):\n",
    "\n",
    "        self.Weights = [np.concatenate((self.Weights[i], np.expand_dims(sess.run(tf.trainable_variables()[i]), -1)), -1)\n",
    "                        for i in range(self.num_weights)]\n",
    "\n",
    "        # The trainable parameters are [q x p] matrices, we expand them to [q x p x 1] in order to stack them\n",
    "        # along the third dimension to the already allocated older variables. We therefore have a list of 6 numpy arrays\n",
    "        # , each numpy array having three dimensions. The last dimension is the one, the individual weight\n",
    "        # matrices are stacked along.\n",
    "\n",
    "    def compute_updates(self):\n",
    "\n",
    "        # To compute the updates, we subtract the global model from each individual weight matrix. Note:\n",
    "        # self.Weights[i] is of size [q x p x m], where m is the number of clients whose matrices are stored.\n",
    "        # global_model['i'] is of size [q x p], in order to broadcast correctly, we have to add a dim.\n",
    "\n",
    "        self.Updates = [self.Weights[i]-np.expand_dims(self.global_model[i], -1) for i in range(self.num_weights)]\n",
    "        self.Weights = None\n",
    "\n",
    "    def compute_norms(self):\n",
    "\n",
    "        # The norms List shall have 6 entries, each of size [1x1xm], we keep the first two dimensions because\n",
    "        # we will later broadcast the Norms onto the Updates of size [q x p x m]\n",
    "\n",
    "        self.Norms = [np.sqrt(np.sum(\n",
    "            np.square(self.Updates[i]), axis=tuple(range(self.Updates[i].ndim)[:-1]),keepdims=True)) for i in range(self.num_weights)]\n",
    "\n",
    "    def clip_updates(self):\n",
    "        self.compute_updates()\n",
    "        self.compute_norms()\n",
    "\n",
    "        # The median is a list of 6 entries, each of size [1x1x1],\n",
    "\n",
    "        self.median = [np.median(self.Norms[i], axis=-1, keepdims=True) for i in range(self.num_weights)]\n",
    "\n",
    "        # The factor is a list of 6 entries, each of size [1x1xm]\n",
    "\n",
    "        factor = [self.Norms[i]/self.median[i] for i in range(self.num_weights)]\n",
    "        for i in range(self.num_weights):\n",
    "            factor[i][factor[i] > 1.0] = 1.0\n",
    "\n",
    "        self.ClippedUpdates = [self.Updates[i]/factor[i] for i in range(self.num_weights)]\n",
    "\n",
    "    def Update_via_GaussianMechanism(self, sess, Acc, FLAGS, Computed_deltas):\n",
    "        self.clip_updates()\n",
    "        self.m = float(self.ClippedUpdates[0].shape[-1])\n",
    "        MeanClippedUpdates = [np.mean(self.ClippedUpdates[i], -1) for i in range(self.num_weights)]\n",
    "\n",
    "        GaussianNoise = [(1.0/self.m * np.random.normal(loc=0.0, scale=float(self.Sigma * self.median[i]), size=MeanClippedUpdates[i].shape)) for i in range(self.num_weights)]\n",
    "        for update, noise, median in zip(MeanClippedUpdates, GaussianNoise, self.median):\n",
    "            print(\"Median\", median)\n",
    "\n",
    "        print(\"m:\", self.m)\n",
    "        Sanitized_Updates = [MeanClippedUpdates[i]+GaussianNoise[i] for i in range(self.num_weights)]\n",
    "\n",
    "        New_weights = [self.global_model[i]+Sanitized_Updates[i] for i in range(self.num_weights)]\n",
    "\n",
    "        New_model = dict(zip(self.keys, New_weights))\n",
    "\n",
    "        t = Acc.accumulate_privacy_spending(0, self.Sigma, self.m)\n",
    "        delta = 1\n",
    "        if FLAGS.record_privacy == True:\n",
    "            if FLAGS.relearn == False:\n",
    "                # I.e. we never learned a complete model before and have therefore never computed all deltas.\n",
    "                for j in range(len(self.keys)):\n",
    "                    sess.run(t)\n",
    "                r = Acc.get_privacy_spent(sess, [FLAGS.eps])\n",
    "                delta = r[0][1]\n",
    "            else:\n",
    "                # I.e. we have computed a complete model before and can reuse the deltas from that time.\n",
    "                delta = Computed_deltas[self.round]\n",
    "        return New_model, delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_placeholder = dict(zip([Vname_to_FeedPname(var) for var in tf.compat.v1.trainable_variables()],\n",
    "                                 [tf.placeholder(name=Vname_to_Pname(var),\n",
    "                                                 shape=var.shape,\n",
    "                                                 dtype=tf.float32)\n",
    "                                  for var in tf.compat.v1.trainable_variables()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = [tf.assign(var, model_placeholder[Vname_to_FeedPname(var)]) for var in\n",
    "                   tf.compat.v1.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The Session graph is empty.  Add operations to the graph before calling run().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-a2193e5fecff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mweights_accountant_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeightsAccountant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1096\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1099\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
     ]
    }
   ],
   "source": [
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init)\n",
    "weights_accountant_2 = WeightsAccountant(sess, model, sigma, real_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model_1 = build_cnn(input_shape=(28, 28, 1))\n",
    "model_2 = build_cnn(input_shape=(28, 28, 1))\n",
    "model_3 = build_cnn(input_shape=(28, 28, 1))\n",
    "model_4 = build_cnn(input_shape=(28, 28, 1))\n",
    "# Compile the model\n",
    "model_1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_4.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1 = model_1.get_weights()\n",
    "weights_2 = model_2.get_weights()\n",
    "weights_3 = model_3.get_weights()\n",
    "weights_4 = model_4.get_weights()\n",
    "acc = WeightsAccountant(weights_1, 100, model_1)\n",
    "acc.append_local_weights(weights_2)\n",
    "acc.append_local_weights(weights_3)\n",
    "acc.append_local_weights(weights_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'sqrt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-47faf1ebbcef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_local_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_update_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_local_clipped_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_noisy_global_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-cb4c4b10daf2>\u001b[0m in \u001b[0;36mcompute_update_norm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         self.Norms = [np.sqrt(np.sum(\n\u001b[0;32m---> 46\u001b[0;31m             np.square(self.Updates[i]), axis=tuple(range(np.array(self.Updates[i]).ndim)[:-1]),keepdims=True)) for i in range(self.num_weights)]\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-cb4c4b10daf2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         self.Norms = [np.sqrt(np.sum(\n\u001b[0;32m---> 46\u001b[0;31m             np.square(self.Updates[i]), axis=tuple(range(np.array(self.Updates[i]).ndim)[:-1]),keepdims=True)) for i in range(self.num_weights)]\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'sqrt'"
     ]
    }
   ],
   "source": [
    "acc.compute_local_updates()\n",
    "acc.compute_update_norm()\n",
    "acc.clip_updates()\n",
    "acc.average_local_clipped_updates()\n",
    "acc.compute_noisy_global_weights(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
